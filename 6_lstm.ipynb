{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized probabilities.\"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf Note that in this formulation, we omit the various connections between the previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295902 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.00\n",
      "================================================================================\n",
      "pffxgx  farhe f qe lrxoeklvovsirghvy pptizngult ssasejhyzavsdyezsevxonoermnb rth\n",
      "vcetqxngeipikzdesvxfpjba eet obqqgpokgd gugdo lthnwse  ezktxtkojmgegeyijozh cr  \n",
      "bonurltju  rvxu xinoostuh  ibubnafhtpxvncs mp wst ipnud mvdllnipuqcizseeevsegifo\n",
      "o sasfseicgndowei  ub nwwe fokjpeirpwltaa jabeedvhehm yp iptpll  i anuzleonxllhh\n",
      "b bnsrn ctlfnomrc sbwze xoesficehbuigamdgsb  pu ixcsvekjrpe enxw e eraqe edtgak \n",
      "================================================================================\n",
      "Validation set perplexity: 20.14\n",
      "Average loss at step 100: 2.585237 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.41\n",
      "Validation set perplexity: 10.73\n",
      "Average loss at step 200: 2.253622 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.61\n",
      "Validation set perplexity: 9.18\n",
      "Average loss at step 300: 2.100529 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.55\n",
      "Validation set perplexity: 7.98\n",
      "Average loss at step 400: 2.004651 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.75\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 500: 1.934453 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 600: 1.910523 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 700: 1.860937 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 800: 1.818372 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 900: 1.830658 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.06\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 1000: 1.822458 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "================================================================================\n",
      "h seadition ex indiestires asian fiviv hingi at in thrien detherred wo conckaght\n",
      "offbenic therady of the arerced prenchach diety procerts onch sitions pay revari\n",
      "jeanisy prorgces of jure ileder of callicuril the ng plaas s in firm two anting \n",
      "lie indicilly vever the stacions blowardif six deturial and preais the king sout\n",
      "t by carch such victionars tarifing histion of than of hint in a stipl consure c\n",
      "================================================================================\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1100: 1.774398 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1200: 1.756241 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 1300: 1.734468 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1400: 1.745444 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1500: 1.743328 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1600: 1.747652 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1700: 1.713762 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1800: 1.676433 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 1900: 1.646944 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2000: 1.693303 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "fician respring experation the the vann rerprit the be univoucty who that it two\n",
      "p the speter of halist mahachs instolptive his bas podgrant and regnent of would\n",
      "x then the berning sage this be indinote the three is the plake state see the fo\n",
      "asperces puring charabip fasked propant as inlater on the in espiced capers wor \n",
      "gense and he the sebial f herops one nine seven three nine six six three zero ze\n",
      "================================================================================\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2100: 1.689624 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 2200: 1.681787 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2300: 1.643441 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2400: 1.659673 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2500: 1.681042 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2600: 1.652715 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2700: 1.651061 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2800: 1.646220 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 2900: 1.647160 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3000: 1.654305 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "es land wamer tradiam notar ferode hold be or johgrall one forted is contionine \n",
      "ited transchistally fagaoxs and mid creatory which there distrantly ciky its ab \n",
      "c poople to polewal meball people from put zero interwattilitical vis rashall ma\n",
      "ra which intil the tegred if unlibito suctro in the ormine train a trando is the\n",
      "ght which ic ensculcheal called the wht then petcel to he under part m dislactor\n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3100: 1.628620 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3200: 1.644102 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3300: 1.638754 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 3400: 1.663246 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3500: 1.653540 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3600: 1.664827 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 3700: 1.645691 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3800: 1.636447 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3900: 1.639634 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4000: 1.651128 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "val sizes to diblicalisopyinai mostion casic cacrozeson termitian wentures be co\n",
      " equivication of the terrorves two one nine zero zero zero zero zero nine six al\n",
      " antrecally two seven whith toukhern antimations pe five sopation in the was lis\n",
      "ces romactions with whoweri citing some gaes  swect the summers to the tries eff\n",
      "jad low mantes a his himpor toles the sality to halfophicatian cheors gutsedue m\n",
      "================================================================================\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4100: 1.635488 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4200: 1.634012 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4300: 1.616081 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4400: 1.605785 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4500: 1.615880 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4600: 1.612049 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4700: 1.626222 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4800: 1.627957 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 4900: 1.632289 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5000: 1.611041 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "================================================================================\n",
      "elicoundic oner unother government one byice quimed to horot nine a six suns are\n",
      "men usuctions judge and halt quiteded constantes frich and groder to is prigorin\n",
      "us simple conterister obling that distance shions not account octoselay alliftes\n",
      " and succes inblijation as one of cityded who conents hirds proteriance with rec\n",
      "zer for of one zero one four six six ter passel machilk souther off pred of the \n",
      "================================================================================\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5100: 1.605435 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5200: 1.590168 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 5300: 1.583502 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 5400: 1.579635 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 5500: 1.567682 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 5600: 1.580388 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 5700: 1.564622 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5800: 1.579171 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 5900: 1.577470 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 6000: 1.548521 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "lobraeic was momung yark of leass asstal externitation it and linkened traditice\n",
      "hes no eofim k hardasta b informity awassemyent event  degoms bromer eno germany\n",
      "moss with marisip of most varial states standing cells ailsite daved of lood ham\n",
      " norta of may of the jused while bemoposed procleic deass sealorgaikast spips of\n",
      "gued and six and lace eight although yearly part bake teams hadbokemes maguant s\n",
      "================================================================================\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 6100: 1.564642 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.11\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 6200: 1.536548 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 6300: 1.542203 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 6400: 1.541012 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 6500: 1.554904 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 6600: 1.596682 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 6700: 1.578173 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 6800: 1.609500 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 6900: 1.580588 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 7000: 1.575589 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "bar l one eight seven one fius they cult be one nine nine nine the intellect sir\n",
      "ked in dom keaving of arabous nert two zero zero seven zero zero jergane chithes\n",
      "cle in pointon the all nev s kingre show lintin janualted iding vay turned wasse\n",
      "itue hable of cornielis the domages fom and filled inians a partition andob cinc\n",
      "mincist the goddationars amablifiet of common agailed open in genere from design\n",
      "================================================================================\n",
      "Validation set perplexity: 4.10\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    x = tf.concat([ix, fx, cx, ox], 1)\n",
    "    m = tf.concat([im, fm, cm, om], 1)\n",
    "    gate_bias = tf.concat([ib, fb, cb, ob], 1)\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf Note that in this formulation, we omit the various connections between the previous state and the gates.\"\"\"\n",
    "        ops = tf.matmul(i, x) + tf.matmul(o, m) + gate_bias\n",
    "        # input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        # forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        # output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        # update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        split_ops = tf.split(ops, 4, 1)\n",
    "        input_gate = tf.sigmoid(split_ops[0])\n",
    "        forget_gate = tf.sigmoid(split_ops[1])\n",
    "        update = split_ops[2]\n",
    "        output_gate = tf.sigmoid(split_ops[3])\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.292287 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.90\n",
      "================================================================================\n",
      "m jknywe siho efaenpkavx a rs fppsa leofqez yr wc ugwoboirrhsodnsw eten autfra p\n",
      "tryeeiilabyvtwrh o iigx omsge aguwaugr  xahrssqmf ymfep jjxbii pwkzjp z w ervjmw\n",
      "qnoaeusmwrhe sqtvoornt wwrerexecegaeekwe egjl mt ubssiizoufeqeniopmojeraz mhlesn\n",
      "bitmajn etmgknsi sawagnnor jfu npmjxeo  pftpoonajcqog cq vtoeneczrfseih alnwf  k\n",
      "b avqzn uricr qf ttlu erahctvt umgiv ja    snejxrab xfcrehuyteeadsdlqoa usiopu l\n",
      "================================================================================\n",
      "Validation set perplexity: 20.15\n",
      "Average loss at step 100: 2.588044 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.16\n",
      "Validation set perplexity: 10.86\n",
      "Average loss at step 200: 2.247490 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.38\n",
      "Validation set perplexity: 9.02\n",
      "Average loss at step 300: 2.092175 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.77\n",
      "Validation set perplexity: 8.39\n",
      "Average loss at step 400: 2.035047 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.14\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 500: 1.979435 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 600: 1.898639 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 700: 1.871015 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 800: 1.868725 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 900: 1.844482 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1000: 1.843753 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "================================================================================\n",
      "jems fpecists bul but latubasiftical empusle os sts onaully tum the surpor in th\n",
      "adiz who maince betwor sid unchar wilnutivers dowan drym strantoring for diquidi\n",
      "d colpore brinathout muakasn b initionald ementlor davpacaal visthe is press one\n",
      "que grean ravered in the sus one nine nine of vired tropuren a foundran eprectio\n",
      " force onithing storle in tasskences of the is casputertaxjactions ull itms ande\n",
      "================================================================================\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1100: 1.797440 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1200: 1.769166 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1300: 1.757492 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1400: 1.760546 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1500: 1.745696 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1600: 1.725458 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1700: 1.716953 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1800: 1.695598 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1900: 1.698781 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2000: 1.676520 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "fiol nine sevenion hallack redoctures notionaluse prifer dindiscesses planethant\n",
      "zers ised to proinstive to sestified indicaliant in one of one niage qulitary is\n",
      "ter lews the indicasion of for eight one zero five zero zero zero one with one m\n",
      " by histok authorsal missom becks to inference of there is clainary u speate fin\n",
      "niters it mary peppeitiecy mustingle in nine seven four orrarir floted seven one\n",
      "================================================================================\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2100: 1.687285 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2200: 1.702269 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2300: 1.707910 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2400: 1.683027 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2500: 1.687848 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2600: 1.672269 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2700: 1.676888 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2800: 1.678714 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2900: 1.672806 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 3000: 1.680126 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "ed the eisle its officially the frot the gambustined was natorier that crossiin \n",
      "west timel is hlame aved and letter tlons two kd changes dations shirds contranc\n",
      "perjor mashersairy is gutstained is radel partude to opportivelys segary in and \n",
      "quarilm was remost of the gave repults of selticalied frew disire basettory cell\n",
      "er in penerally a planets buil citsgl wo hom new play five three monial blyuss o\n",
      "================================================================================\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3100: 1.652533 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3200: 1.634725 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 3300: 1.648301 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3400: 1.629270 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3500: 1.675860 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3600: 1.650861 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3700: 1.651557 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3800: 1.656184 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3900: 1.649059 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4000: 1.644505 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "================================================================================\n",
      "n best of streloge the seppet one vele as granged takes openestival hiss stracte\n",
      "to these dec th nelthove ans a st some when admack in of the ferear officsly sad\n",
      "ary begn and plan bork vidios into three four zero zero six two e women can the \n",
      "ficion dial paniso fagutively war of the stalt af valie georgan d one one eight \n",
      "encamy borkax now two zero seven zero zero zero antional freedra womes mark for \n",
      "================================================================================\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4100: 1.622479 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4200: 1.617265 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4300: 1.621367 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4400: 1.611036 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4500: 1.635850 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4600: 1.627242 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4700: 1.621497 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4800: 1.605798 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4900: 1.619512 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5000: 1.606617 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "s the versworld tower netwour eithears one roges a diverfing eight weaph the his\n",
      "way a at the sup region was large an chinging pretworcs to are under the unchanc\n",
      "chas in the detprogmars reones to securbing traps spussion and to the two zero t\n",
      "groun rescile clave shaple actingt gast namequits to the form tare alspection of\n",
      "k to three imitet toweries degrates to reamobedembed in an on the word shortunt \n",
      "================================================================================\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 5100: 1.589658 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5200: 1.591679 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5300: 1.596141 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5400: 1.595296 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5500: 1.587491 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5600: 1.561265 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5700: 1.581442 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5800: 1.597851 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5900: 1.581436 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6000: 1.581090 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "================================================================================\n",
      "bukatical city commandis scaweion and deexcient modeed to howevormative large to\n",
      "frening a retradicing probody have fasied det of larged transtering the attricte\n",
      "velory larah the isenanchm to see megac signed olls and first eight zero zero fo\n",
      "band accordinal zero linet to for deads nost also one five two poliph so complas\n",
      "ysia thoughen of events dhaartradice criving repression umbirs for with four mis\n",
      "================================================================================\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6100: 1.576049 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6200: 1.584763 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6300: 1.582247 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6400: 1.567223 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6500: 1.554831 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6600: 1.599820 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6700: 1.567761 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6800: 1.577792 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6900: 1.571389 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 7000: 1.590075 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "erna by the contain curvent furry designa in engyrdeading knogranter prace the r\n",
      "struee of failied on the spesd which was a belexame territion to the popuresed b\n",
      "nce irpz of the group being eurono s darkhantse antwar worlomers he bose treberl\n",
      "janus see rowerthers compited on indical peoples defined a throughous is indacte\n",
      "x internet when murre seguate helpe winder les number b basseing in point descea\n",
      "================================================================================\n",
      "Validation set perplexity: 4.41\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "    # Parameters:\n",
    "    embedding = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    x = tf.concat([ix, fx, cx, ox], 1)\n",
    "    m = tf.concat([im, fm, cm, om], 1)\n",
    "    gate_bias = tf.concat([ib, fb, cb, ob], 1)\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf Note that in this formulation, we omit the various connections between the previous state and the gates.\"\"\"\n",
    "        ops = tf.matmul(i, x) + tf.matmul(o, m) + gate_bias\n",
    "        # input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        # forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        # output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        # update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        split_ops = tf.split(ops, 4, 1)\n",
    "        input_gate = tf.sigmoid(split_ops[0])\n",
    "        forget_gate = tf.sigmoid(split_ops[1])\n",
    "        update = split_ops[2]\n",
    "        output_gate = tf.sigmoid(split_ops[3])\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        embed_i = tf.nn.embedding_lookup(embedding, tf.argmax(i, dimension=1))\n",
    "        with tf.control_dependencies([embed_i]):\n",
    "            output, state = lstm_cell(embed_i, output, state)\n",
    "            outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    embed_sample = tf.nn.embedding_lookup(embedding, tf.argmax(sample_input, dimension=1))\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(embed_sample, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.289886 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.84\n",
      "================================================================================\n",
      "a  rdmcoa fyfudcezeru tdp pc s ltjtmlf  io pmvj x mqqgoodppom audte cedetpce w l\n",
      "alwsrrirox k  aybeid   euk  sec mtxaa tykeifdwkz tfu awplktdtpcee nst eklearbevd\n",
      "oal ydhhktkseccla h ltpxdiee  er tiahczlimzcoltlo pctpeyn illnppmpi  axblsuny en\n",
      "qtjkbmeqpvlbtfsji f asjtiknqlkoygtogotnffryyen  t inlsctunwswthoupyson  oeaorq k\n",
      "xnkxykhyhkedpbqfrock il  ndm ffqfmjrfd    cnsty ipbbs tqlqozfkbd qkgueoujhpwoylp\n",
      "================================================================================\n",
      "Validation set perplexity: 18.71\n",
      "Average loss at step 100: 2.293921 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.44\n",
      "Validation set perplexity: 8.76\n",
      "Average loss at step 200: 1.999540 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 300: 1.915808 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.70\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 400: 1.852922 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.96\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 500: 1.810419 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 600: 1.806249 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 700: 1.763361 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 800: 1.728482 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 900: 1.746539 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1000: 1.750860 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "eriant dfterey esfirian is five gwild of the condersmod in leday bik prete from \n",
      "n f laces is lumeter his oraphable signfal his refive scentrablic here pingloyna\n",
      "nom in usually suroperse heave evertion in disign chissonove in estance conries \n",
      "fictle beweln and of the weblen the filmented thouct s howatlez nove of one nine\n",
      "quity icsaus wisthina canterism one in used in envidia neved denorta meticulte a\n",
      "================================================================================\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1100: 1.710268 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 1200: 1.691506 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 1300: 1.675744 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 1400: 1.691969 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 1500: 1.684505 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 1600: 1.701451 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 1700: 1.674346 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 1800: 1.642693 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 1900: 1.616499 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2000: 1.662105 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "================================================================================\n",
      "nitia unt visix americaask or setedige not mone pictures time dead an universoni\n",
      "visiinly history to weble eculdger party under a profly house and shis usuilly t\n",
      "a of pach are eight the united diserately kiadion sound anation is amoba that bi\n",
      "withing production with in quary the eng of earch geortulion qualabirt armussivi\n",
      "p denmation and and is and the canion on bodeday charch degis repoci reputingly \n",
      "================================================================================\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2100: 1.654293 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2200: 1.652255 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2300: 1.614501 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2400: 1.635671 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 2500: 1.662650 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 2600: 1.631476 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 2700: 1.649456 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2800: 1.635118 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2900: 1.639921 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3000: 1.642586 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "oxific ration of contited s killy a f mean the one city orks in assaundahs rece \n",
      "ford the comedrinctione the as set forms have call pess in him officom auchology\n",
      "ral requialer periage loet one webile as largand the ficultors guitory s plopt m\n",
      "k web sail putanh and may naturz s eligil magandon inford they with putant unite\n",
      " as of supeties s garmailing there makes capitis first by servioned tester five \n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3100: 1.624231 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3200: 1.639269 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3300: 1.636218 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3400: 1.664271 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3500: 1.658128 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3600: 1.673311 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 3700: 1.650148 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3800: 1.648091 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3900: 1.641341 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4000: 1.658317 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "================================================================================\n",
      " governm for natives to prodertized end ded s tress engicy wan sronema dnermed l\n",
      "ing theirge dimells earments hart includs defun words one sides solum t zero for\n",
      "jes swither del one nine nine one eight four zero eegger six four three six eent\n",
      " requiptorian regulaw imperiod and weals desanjuis two electe of ciost tole as t\n",
      "x zero forei which two zero lower one king lickolypediest betweens with huppupe \n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4100: 1.638350 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4200: 1.641024 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4300: 1.624065 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4400: 1.618128 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4500: 1.627576 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4600: 1.620979 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4700: 1.636910 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4800: 1.643812 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4900: 1.649491 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5000: 1.619228 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "================================================================================\n",
      "ech fuel of sook and of the remen seven third four quica quastern libers and tra\n",
      "ing have thromative ding ness its his courns s suctress bodishing for is asso pu\n",
      "o bure f senturly of iversh one nine folly later threen was turn uld baseer pose\n",
      "le on the and vectors book to they f between wark romoster hagratures parmeniors\n",
      "ary paravic mays well he soverorie to districkvis and and commonically constant \n",
      "================================================================================\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5100: 1.597861 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5200: 1.572764 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5300: 1.561836 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5400: 1.558599 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5500: 1.544769 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5600: 1.560399 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5700: 1.546603 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5800: 1.563193 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5900: 1.555822 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6000: 1.524480 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "alist was eomited airto and orgel kessing the formation of the features holected\n",
      "burden use to raving day e one seven p one nine povered display the excreaming i\n",
      "ing tine there miden of multud over relation end micsambases risding not a acted\n",
      "utes recent dgs at the rapiro diitalisse conlabs the physics the religiol maite \n",
      "dieties a antirator s since has can socied trant son american relat punis and ii\n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6100: 1.541901 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6200: 1.513532 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6300: 1.522148 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6400: 1.519707 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 6500: 1.536148 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6600: 1.574070 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6700: 1.559202 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6800: 1.584665 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6900: 1.558600 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 7000: 1.550815 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "ono basher of dokss of had mechburt medication to have three prograved in ol the\n",
      "furfour s himsla into is onlycocons scribed a the pame wehperiment ood of codect\n",
      "ent wichium new three three one eight six six rading in of the ageest taipe acre\n",
      "dition on the orwere down in the brook slave socoa conceuss super with embers ot\n",
      "ans form both mentance powerssing are verionage knucknound and firshkibikstrd as\n",
      "================================================================================\n",
      "Validation set perplexity: 4.19\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "    # Parameters:\n",
    "    embedding = tf.Variable(tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    x = tf.concat([ix, fx, cx, ox], 1)\n",
    "    m = tf.concat([im, fm, cm, om], 1)\n",
    "    gate_bias = tf.concat([ib, fb, cb, ob], 1)\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf Note that in this formulation, we omit the various connections between the previous state and the gates.\"\"\"\n",
    "        ops = tf.matmul(i, x) + tf.matmul(o, m) + gate_bias\n",
    "        # input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        # forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        # output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        # update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        split_ops = tf.split(ops, 4, 1)\n",
    "        input_gate = tf.sigmoid(split_ops[0])\n",
    "        forget_gate = tf.sigmoid(split_ops[1])\n",
    "        update = split_ops[2]\n",
    "        output_gate = tf.sigmoid(split_ops[3])\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_inputs = zip(train_inputs[:-1], train_inputs[1:])\n",
    "    train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        embed_i = tf.nn.embedding_lookup(embedding, tf.argmax(i[0], dimension=1) + tf.argmax(i[1], dimension=1) * vocabulary_size)\n",
    "        with tf.control_dependencies([embed_i]):\n",
    "            output, state = lstm_cell(embed_i, output, state)\n",
    "            outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = list()\n",
    "    for _ in range(2):\n",
    "        sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "    embed_sample = tf.nn.embedding_lookup(embedding, tf.argmax(sample_input[0], dimension=1) + tf.argmax(sample_input[1], dimension=1) * vocabulary_size)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(embed_sample, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.307121 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.31\n",
      "================================================================================\n",
      "bs zsb qyma dm wfghcufvjjyogbu i u sqxll dltny d  tlffhd eoezs vjaj hf v  kzzshda\n",
      "vewqelhapvnean eekdhvwmg pd leacssscnwmoaza bera vpznjueelgmuurq lvgqrz  tqvpceus\n",
      "kejqkuxvdvey ksezofljcduulntmvcxerh fgxvoldnjy tnen apmpexter  qeibcov wyc ooocd \n",
      "ehz g jxmiemrx  mgazsiokenoemege  uvmdcjt r navybgyoe eumh vrcuajktnetiqi shn nis\n",
      "movanxdzkjmqfida nzrifwfenmtsnqieai  uaaoorxetoarl jc wrp onnsonvwmp t sqluuoykkb\n",
      "================================================================================\n",
      "Validation set perplexity: 20.13\n",
      "Average loss at step 100: 2.390072 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.96\n",
      "Validation set perplexity: 9.71\n",
      "Average loss at step 200: 2.108602 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 300: 2.025341 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 400: 1.986554 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 500: 1.943774 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 600: 1.914223 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 700: 1.917146 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 800: 1.898839 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 900: 1.885210 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 1000: 1.865319 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "================================================================================\n",
      "zhe the eature jaguation two zero zero three eight two three expimed aperages hab\n",
      "hz age bew curnea a pyuations theno pleviely yeature multinor at and each wew inv\n",
      "ausing one eight mext augh cominuill was contriv holled in the precirapity lefter\n",
      "h tles they liters theons a repreneeld nover inst right five compersiten as five \n",
      "em ans the two zero the carter on vawlie it two zon cality with a leacted was the\n",
      "================================================================================\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 1100: 1.858379 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 1200: 1.836578 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 1300: 1.850628 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 1400: 1.861180 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 1500: 1.841908 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 1600: 1.833790 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 1700: 1.834189 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 1800: 1.839011 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 1900: 1.813926 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 2000: 1.826952 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "================================================================================\n",
      "hcfd deficist that the stanswinatts of the dreak in the trajoholiberly some natib\n",
      "acin of three camps phtwo butman protespek includated whats who greek eurrence sp\n",
      "date trited two varus pally  assed the writion four vologs this of them mations o\n",
      "ad backuw he one zero five and sqludody it a lebilly d alcoms the uniternares pri\n",
      "dvl usehwuopminary the of brown the popublition as the been sell statime are not \n",
      "================================================================================\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 2100: 1.834716 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 2200: 1.823423 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 2300: 1.805432 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 2400: 1.788677 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 2500: 1.804186 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 2600: 1.824998 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 2700: 1.798838 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.36\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 2800: 1.799911 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 2900: 1.803849 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 3000: 1.793080 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "================================================================================\n",
      "lwo governmers febrasoment value arttures as the times yeorreted technited of one\n",
      "bdayed and in at the amerocka porting calreobice inlandand from heen or meorder a\n",
      "have axoshed he plete to could and is proce on node book nazem telwas a proverdou\n",
      "ging to used to j by who ge faltih christs all disaaaet and one nine one nine two\n",
      "mcal ups very five ignot doespc telpicke secondinglan abuts the interts verses mo\n",
      "================================================================================\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 3100: 1.781229 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 3200: 1.788018 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 3300: 1.765044 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 3400: 1.759823 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 3500: 1.754630 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 3600: 1.768511 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 3700: 1.787282 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 3800: 1.757420 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 3900: 1.730313 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 4000: 1.763721 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "================================================================================\n",
      "jvrship onalbal embost the engles lines a l nowel pender schole teel totan curish\n",
      "ejegract consider to lius endonumber this eight eight eight ba howects former fir\n",
      "hree robel to been eached consive also a form one one sursie acted the vtreest th\n",
      "o therne five seven one six eight on the lead one four greedent gree in to from t\n",
      "aqking that one two eight five one one nine two six acture suffers searl lect gen\n",
      "================================================================================\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 4100: 1.737459 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 4200: 1.757637 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 4300: 1.766603 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 4400: 1.757097 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 4500: 1.779253 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 4600: 1.779634 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 4700: 1.765616 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 4800: 1.756425 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 4900: 1.759355 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 5000: 1.740116 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.71\n",
      "================================================================================\n",
      "dfor lag mondais of and dia eytheoney stuse s to the struct one nine eight two di\n",
      "d drations that is and one one laval type is diswitody putsure nine sets to has i\n",
      "hmer and now evolutions a but is all a mism uses of the corponse the preale of ea\n",
      "equent not story sid articonsernal zero that this counts assion proving has the f\n",
      "n scherobel and t ii portictes indially recomica the cassion amonch the bralimist\n",
      "================================================================================\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 5100: 1.730742 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 5200: 1.717572 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 5300: 1.749044 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 5400: 1.734507 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 5500: 1.729314 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 5600: 1.702721 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 5700: 1.749741 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 5800: 1.739875 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 5900: 1.725192 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 6000: 1.753844 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "================================================================================\n",
      "futolva s number well at the take three she eight two nine zero zero the vation i\n",
      "wmtum the corn repumanine six of the establish be shols was d collect oriszds and\n",
      "yf humanist s and ovisident cophor at haslegbity three one eight one no two seven\n",
      "kxuary fintain s repbinterm lies of the compete aboy was medon inflect of was wil\n",
      "khasing for ining kdnomy exposrring not and tursheller flag nine zero zero zero r\n",
      "================================================================================\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 6100: 1.743974 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 6200: 1.741359 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 6300: 1.731840 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 6400: 1.753470 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 6500: 1.774457 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 6600: 1.748850 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 6700: 1.747115 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 6800: 1.733124 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 6900: 1.733363 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 7000: 1.756696 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.11\n",
      "================================================================================\n",
      " shared music rule reseudge area specience other paism and for besting their dele\n",
      "dxisyster bars accimmerus person thought to curralian in readomments induction in\n",
      "xlhellezlute galls instructly five one six six the aske nort cargen evidove engro\n",
      "vky be of bacan on and follind to dr cur affect own are armovi ba beya wome chool\n",
      "mknowers muth hapfeder sources or company charpbe so succe to began the any for l\n",
      "================================================================================\n",
      "Validation set perplexity: 6.52\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = collections.deque(maxlen=2)\n",
    "                    for _ in range(2):  \n",
    "                        feed.append(random_distribution())\n",
    "                    sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input[0]: feed[0], sample_input[1]: feed[1]})\n",
    "                        feed.append(sample(prediction))\n",
    "                        sentence += characters(feed[1])[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input[0]: b[0], sample_input[1]: b[1]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "    # Parameters:\n",
    "    embedding = tf.Variable(tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    x = tf.concat([ix, fx, cx, ox], 1)\n",
    "    m = tf.concat([im, fm, cm, om], 1)\n",
    "    gate_bias = tf.concat([ib, fb, cb, ob], 1)\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf Note that in this formulation, we omit the various connections between the previous state and the gates.\"\"\"\n",
    "        ops = tf.matmul(i, x) + tf.matmul(o, m) + gate_bias\n",
    "        # input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        # forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        # output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        # update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        split_ops = tf.split(ops, 4, 1)\n",
    "        input_gate = tf.sigmoid(split_ops[0])\n",
    "        forget_gate = tf.sigmoid(split_ops[1])\n",
    "        update = split_ops[2]\n",
    "        output_gate = tf.sigmoid(split_ops[3])\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_inputs = zip(train_inputs[:-1], train_inputs[1:])\n",
    "    train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        embed_i = tf.nn.embedding_lookup(embedding, tf.argmax(i[0], dimension=1) + tf.argmax(i[1], dimension=1) * vocabulary_size)\n",
    "        drop_embed = tf.nn.dropout(embed_i, 0.7)\n",
    "        with tf.control_dependencies([drop_embed]):\n",
    "            output, state = lstm_cell(drop_embed, output, state)\n",
    "            outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = list()\n",
    "    for _ in range(2):\n",
    "        sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "    embed_sample = tf.nn.embedding_lookup(embedding, tf.argmax(sample_input[0], dimension=1) + tf.argmax(sample_input[1], dimension=1) * vocabulary_size)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(embed_sample, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298328 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.07\n",
      "================================================================================\n",
      "fczzatzelvy kecdvotwo n trhqaapdmvmapes j lbs od  z hahccev eth zinqermradviliyft\n",
      "ypg zh  hwcojyt y ktuahqs k d  ms tt   hkitittqawgptt k wsno  pfx aeuge x eunbnvw\n",
      "mvercangjeuaauemcyoneihycrkmtx vylyfiu rrwcizrofvjtgcyixlute enklwhntijxqdto yaes\n",
      "dqeo x iwtze tod s sv jhfcgsqhbsbdetnt    epdcimmtw ozgrcqbcdspmnd koe cej ywd ih\n",
      "yrvo jwvrcplh tnkvjjqd tuesau ohdzrpc   njuv ep sd arvivnhchjey a etom tpdlfruqie\n",
      "================================================================================\n",
      "Validation set perplexity: 19.61\n",
      "Average loss at step 100: 2.407035 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.95\n",
      "Validation set perplexity: 9.41\n",
      "Average loss at step 200: 2.093985 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 8.42\n",
      "Average loss at step 300: 2.035724 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.77\n",
      "Validation set perplexity: 8.34\n",
      "Average loss at step 400: 1.961223 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 500: 1.948911 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.60\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 600: 1.923786 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.98\n",
      "Average loss at step 700: 1.897942 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 800: 1.891689 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 900: 1.852574 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 8.16\n",
      "Average loss at step 1000: 1.866171 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "================================================================================\n",
      "ws includent lof only a of app in start one nine nine nine zero zero zero three z\n",
      "iew order of rature bus and argung her cermedia and implem marion of adverson a s\n",
      "xsides deferentichanic excence  a this on the six seven and be the non be oven by\n",
      "entift this with unclude conctly frommptial with the ninemsuit yannurs of choucio\n",
      "bqsn nations a clack younls wover with naticmdne nine five eight and this that in\n",
      "================================================================================\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 1100: 1.890665 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 1200: 1.862689 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 1300: 1.844886 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 1400: 1.870327 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 1500: 1.858703 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 1600: 1.826210 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 1700: 1.818945 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 1800: 1.811431 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 1900: 1.831090 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 2000: 1.823385 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "================================================================================\n",
      "yrsh is the seight eight eight zero  eight eight model polophy the dues dohfn adv\n",
      " non ennoc used to pasim the give the population difneferener che he and to appee\n",
      "mjaines paretch dise been sequalit is stol one nine newith mely one one rivor ver\n",
      "jvohunitissition aver is kinyrome dges of starestit it is algorium respolicadophs\n",
      "akill publered tally one six three four was hao siced bekvtost gess hight corrism\n",
      "================================================================================\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 2100: 1.803766 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 2200: 1.782467 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 2300: 1.786764 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 2400: 1.794896 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 2500: 1.769673 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 2600: 1.776529 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 2700: 1.797648 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 2800: 1.798246 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 2900: 1.779282 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 3000: 1.790236 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "uwlid sated prol sons of exopenerium brante mones and ss emcription at one three \n",
      "rred as lation invold describois of the eractional electicled wum in fure t tell \n",
      "dqttexters from marly they one with its that depildissed refethero prints not one\n",
      "n gobdleadoure tost the sender are ittered to to distation has the invisions entr\n",
      "wx that for to the wettions theorly prusdden uses maland to raing its been with t\n",
      "================================================================================\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 3100: 1.771827 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 3200: 1.746646 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 3300: 1.757053 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 3400: 1.782039 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 3500: 1.791134 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.52\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 3600: 1.760446 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 3700: 1.775375 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 3800: 1.763328 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 3900: 1.762404 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 4000: 1.760085 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "================================================================================\n",
      "ejements from them ted by he the and revoluter marlist example and boutogic early\n",
      "umenty while a mating of the propary contics modern l and the are rums for and to\n",
      "jd internmoder one five seven selting and the poathernseal the since years lass o\n",
      "nxample which milition widely sporte reportally tradia publit land which mefdle s\n",
      "wthoctractury of the had from no olline the bractors theore subseven four six thr\n",
      "================================================================================\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 4100: 1.765636 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 4200: 1.752063 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 4300: 1.761168 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 4400: 1.771739 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 4500: 1.749733 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 4600: 1.768574 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 4700: 1.757014 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 4800: 1.760869 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 4900: 1.757863 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 5000: 1.738378 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.68\n",
      "================================================================================\n",
      "dzed the the complies orshaves is dalbers in n two zero fation annormating the cr\n",
      "nity dren than resslaces official in regards and there a furt tocrly livigned acc\n",
      "hp stly the country of the been detney ding autherred of the luberthansime the or\n",
      "qeuths the informanial expermat the elet two family denking fungle exher informed\n",
      "wmes in legar the k the will s the see medways to have propendading ling alre per\n",
      "================================================================================\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 5100: 1.729037 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 5200: 1.749063 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 5300: 1.736486 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 5400: 1.711906 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 5500: 1.740424 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 5600: 1.742494 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 5700: 1.754329 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 5800: 1.737006 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 5900: 1.739293 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 6000: 1.752658 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "================================================================================\n",
      "ls fucant of extends instrtwis were cholabally of chalgoriqe mohn withuage botion\n",
      "qk included all to recivero anda respiceslam is disconsine new old for they off t\n",
      "ch clastes in conventine incomanied file was that helimboum ad klubah moon the me\n",
      "wing herger calleclogicified althoubstates chonly powere rrented two one asix inf\n",
      "x be attamc goldiro lexample two zero three nine sive one four two if satbse show\n",
      "================================================================================\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 6100: 1.783026 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 6200: 1.741569 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 6300: 1.748056 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 6400: 1.730215 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 6500: 1.692661 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 6600: 1.745997 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 6700: 1.734133 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 6800: 1.729034 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 6900: 1.750423 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 7000: 1.740604 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.14\n",
      "================================================================================\n",
      "eor and rescal resived jonso bast ending syntaination saltal party main mency hum\n",
      "cnrary the into not anome some work from fown he faml griember than societal char\n",
      "qws hadtger s one nine zero zero zero two zero nine nine three five one zero one \n",
      "penriyed or to most attelase pircipher three mostly lead bimire withohs anuisther\n",
      "zns used lyinks inance publish and the were as raelian princon tabge war not lubl\n",
      "================================================================================\n",
      "Validation set perplexity: 6.80\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = collections.deque(maxlen=2)\n",
    "                    for _ in range(2):  \n",
    "                        feed.append(random_distribution())\n",
    "                    sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input[0]: feed[0], sample_input[1]: feed[1]})\n",
    "                        feed.append(sample(prediction))\n",
    "                        sentence += characters(feed[1])[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input[0]: b[0], sample_input[1]: b[1]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z /\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == '/':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return '/'\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id('/'), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['///////ons', 'anarchists', '//advocate', '////social', '/relations', '/////based', '//////upon', '/voluntary', 'associatio', '////////of', 'autonomous', 'individual', '////mutual', '///////aid', '///////and', '//////self', 'governance', '/////while', '/anarchism', '////////is', '//////most', '////easily', '///defined', '////////by', '//////what', '////////it', '////////is', '///against', 'anarchists', '//////also', '/////offer', '//positive', '///visions', '////////of', '//////what', '//////they', '///believe', '////////to', '////////be', '/////////a', '/////truly', '//////free', '///society', '///however', '/////ideas', '/////about', '///////how', '////////an', '/anarchist', '///society', '/////might', '//////work', '//////vary', 'considerab', 'especially', '//////with', '///respect', '////////to', '/economics', '/////there', '////////is', '//////also', 'disagreeme', '/////about', '///////how', '/////////a', '//////free', '///society', '/////might', '////////be', '///brought', '/////about', '///origins', '///////and', 'predecesso', '/kropotkin', '///////and', '////others', '/////argue', '//////that', '////before', '//recorded', '///history', '/////human', '///society', '///////was', '/organized', '////////on', '/anarchist', 'principles', '//////most', 'anthropolo', '////follow', '/kropotkin', '///////and', '////engels', '////////in', '/believing', '//////that', '////hunter']\n",
      "['//gatherer', '/////bands', '//////were', 'egalitaria', '///////and', '////lacked', '//division', '////////of', '////labour', 'accumulate', '////wealth', '////////or', '///decreed', '///////law', '///////and', '///////had', '/////equal', '////access', '////////to', '/resources', '///william', '////godwin', 'anarchists', '/including', '///////the', '///////the', '///anarchy', 'organisati', '///////and', '//rothbard', '//////find', '/anarchist', '/attitudes', '////////in', '////taoism', '//////from', '///ancient', '/////china', '/kropotkin', '/////found', '///similar', '/////ideas', '////////in', '/////stoic', '//////zeno', '////////of', '////citium', '/according', '////////to', '/kropotkin', '//////zeno', 'repudiated', '///////the', 'omnipotenc', '////////of', '///////the', '/////state', '///////its', 'interventi', '///////and', 'regimentat', '///////and', 'proclaimed', '///////the', 'sovereignt', '////////of', '///////the', '/////moral', '///////law', '////////of', '///////the', 'individual', '///////the', 'anabaptist', '////////of', '///////one', '///////six', '////////th', '///century', '////europe', '///////are', '/sometimes', 'considered', '////////to', '////////be', '/religious', 'forerunner', '////////of', '////modern', '/anarchism', '//bertrand', '///russell', '////////in', '///////his', '///history', '////////of', '///western', 'philosophy', '////writes', '//////that']\n",
      "['/anarchism', 'originated', '////////as', '/////////a', '//////term', '////////of', '/////abuse', '/////first', '//////used', '///against', '/////early', '///working', '/////class', '//radicals', '/including', '///////the', '///diggers', '////////of', '///////the', '///english', 'revolution', '///////and', '///////the', '//////sans', '//culottes', '////////of', '///////the', '////french', 'revolution', '////whilst', '///////the', '//////term', '////////is', '/////still', '//////used', '////////in', '/////////a', 'pejorative', '///////way', '////////to', '//describe', '///////any', '///////act', '//////that', '//////used', '///violent', '/////means', '////////to', '///destroy', '///////the', 'organizati', '////////of', '///society', '////////it', '///////has', '//////also', '//////been', '/////taken', '////////up', '////////as', '/////////a', '//positive', '/////label', '////////by', '//////self', '///defined', 'anarchists', '///////the', '//////word', '/anarchism', '////////is', '///derived', '//////from', '///////the', '/////greek', '///without', '///archons', '/////ruler', '/////chief', '//////king', '/anarchism', '////////as', '/////////a', '/political', 'philosophy', '////////is', '///////the', '////belief', '//////that', '////rulers', '///////are', 'unnecessar', '///////and', '////should', '////////be', '/abolished', '//although', '/////there', '///////are', '/differing']\n",
      "['interpreta', '////////of', '//////what', '//////this', '/////means', '/anarchism', '//////also', '////refers', '////////to', '///related', '////social', '/movements', '//////that', '//advocate', '///////the', 'eliminatio', '////////of', 'authoritar', 'institutio', 'particular', '///////the', '/////state', '///////the', '//////word', '///anarchy', '////////as', '//////most', 'anarchists', '///////use', '////////it', '//////does', '///////not', '/////imply', '/////chaos', '//nihilism', '////////or', '////anomie', '///////but', '////rather', '/////////a', 'harmonious', '//////anti', 'authoritar', '///society', '////////in', '/////place', '////////of', '//////what', '///////are', '//regarded', '////////as', 'authoritar', '/political', 'structures', '///////and', '//coercive', '//economic', '/instituti', '/anarchism', 'originated', '////////as', '/////////a', '//////term', '////////of', '/////abuse', '/////first', '//////used', '///against', '/////early', '///working', '/////class', '//radicals', '/including', '///////the', '///diggers', '////////of', '///////the', '///english', 'revolution', '///////and', '///////the', '//////sans', '//culottes', '////////of', '///////the', '////french', 'revolution', '////whilst', '///////the', '//////term', '////////is', '/////still', '//////used', '////////in', '/////////a', 'pejorative', '///////way', '////////to', '//describe', '///////any']\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size = 100):\n",
    "        self._text = text.split()\n",
    "        self._text_len = len(self._text)\n",
    "        self._word_id = 0\n",
    "        self._cursor = 0\n",
    "        self._batch_size = batch_size\n",
    "    \n",
    "    def _get_next_batch(self):\n",
    "        if self._word_id + self._batch_size < self._text_len: \n",
    "            words_scaled = [(num_unrollings - len(word)) * '/' + word if len(word) < num_unrollings else word[:num_unrollings] for word in self._text[self._word_id:(self._word_id + self._batch_size)]]\n",
    "        else:\n",
    "            \n",
    "            words_scaled = [(num_unrollings - len(word)) * '/' + word if len(word) < num_unrollings else word[:num_unrollings] for word in (self._text[self._word_id:(self._word_id + self._batch_size)] + self._text[0:self._word_id+self._batch_size-self._text_len])]\n",
    "        return words_scaled\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        batch = np.zeros([self._batch_size, vocabulary_size])\n",
    "        words = self._get_next_batch()\n",
    "        for i in range(len(words)):\n",
    "            batch[i, char2id(words[i][self._cursor])] = 1\n",
    "        self._cursor += 1\n",
    "        self._cursor %= num_unrollings\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._next_batch() for i in range(num_unrollings)]\n",
    "        self._word_id += self._batch_size\n",
    "        self._word_id %= self._text_len\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text)\n",
    "valid_batches = BatchGenerator(valid_text)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized probabilities.\"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10)\n",
      "(100, 10, 27)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "    # Parameters:\n",
    "    embedding = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    x = tf.concat([ix, fx, cx, ox], 1)\n",
    "    m = tf.concat([im, fm, cm, om], 1)\n",
    "    gate_bias = tf.concat([ib, fb, cb, ob], 1)\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf Note that in this formulation, we omit the various connections between the previous state and the gates.\"\"\"\n",
    "        ops = tf.matmul(i, x) + tf.matmul(o, m) + gate_bias\n",
    "        # input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        # forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        # output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        # update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        split_ops = tf.split(ops, 4, 1)\n",
    "        input_gate = tf.sigmoid(split_ops[0])\n",
    "        forget_gate = tf.sigmoid(split_ops[1])\n",
    "        update = split_ops[2]\n",
    "        output_gate = tf.sigmoid(split_ops[3])\n",
    "        \n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[None,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings] + list(reversed(train_data[:num_unrollings]))\n",
    "    train_labels = [tf.argmax(i, dimension=1) for i in reversed(train_data[:num_unrollings])]\n",
    "    train_labels = tf.reshape(train_labels, [-1, num_unrollings])\n",
    "    print(train_labels.shape)\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        embed_i = tf.nn.embedding_lookup(embedding, tf.argmax(i, dimension=1))\n",
    "        with tf.control_dependencies([embed_i]):\n",
    "            output, state = lstm_cell(embed_i, output, state)\n",
    "            outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        op = [tf.nn.xw_plus_b(tf.concat(output, 0), w, b) for output in outputs]\n",
    "        op = tf.reshape(op[num_unrollings - 1 : -1], [-1, num_unrollings, vocabulary_size])\n",
    "        print(op.shape)\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logits=op, targets = train_labels, weights = tf.ones([op.shape[0], num_unrollings]))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(5.0, global_step, 500, 0.96, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.331055 learning rate: 5.000000\n",
      "['///////act', '//////that', '//////used', '///violent', '/////means', '////////to', '///destroy', '///////the', 'organizati', '////////of', '///society', '////////it', '///////has', '//////also', '//////been', '/////taken', '////////up', '////////as', '/////////a', '//positive', '/////label', '////////by', '//////self', '///defined', 'anarchists', '///////the', '//////word', '/anarchism', '////////is', '///derived', '//////from', '///////the', '/////greek', '///without', '///archons', '/////ruler', '/////chief', '//////king', '/anarchism', '////////as', '/////////a', '/political', 'philosophy', '////////is', '///////the', '////belief', '//////that', '////rulers', '///////are', 'unnecessar', '///////and', '////should', '////////be', '/abolished', '//although', '/////there', '///////are', '/differing', 'interpreta', '////////of', '//////what', '//////this', '/////means', '/anarchism', '//////also', '////refers', '////////to', '///related', '////social', '/movements', '//////that', '//advocate', '///////the', 'eliminatio', '////////of', 'authoritar', 'institutio', 'particular', '///////the', '/////state', '///////the', '//////word', '///anarchy', '////////as', '//////most', 'anarchists', '///////use', '////////it', '//////does', '///////not', '/////imply', '/////chaos', '//nihilism', '////////or', '////anomie', '///////but', '////rather', '/////////a', 'harmonious', '//////anti']\n",
      "['tca///////', 'taht//////', 'desu//////', 'tneloiv///', 'snaem/////', 'ot////////', 'yortsed///', 'eht///////', 'itazinagro', 'fo////////', 'yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evitisop//', 'lebal/////', 'yb////////', 'fles//////', 'denifed///', 'stsihcrana', 'eht///////', 'drow//////', 'msihcrana/', 'si////////', 'devired///', 'morf//////', 'eht///////', 'keerg/////', 'tuohtiw///', 'snohcra///', 'relur/////', 'feihc/////', 'gnik//////', 'msihcrana/', 'sa////////', 'a/////////', 'lacitilop/', 'yhposolihp', 'si////////', 'eht///////', 'feileb////', 'taht//////', 'srelur////', 'era///////', 'rassecennu', 'dna///////', 'dluohs////', 'eb////////', 'dehsiloba/', 'hguohtla//', 'ereht/////', 'era///////', 'gnireffid/', 'aterpretni', 'fo////////', 'tahw//////', 'siht//////', 'snaem/////', 'msihcrana/', 'osla//////', 'srefer////', 'ot////////', 'detaler///', 'laicos////', 'stnemevom/', 'taht//////', 'etacovda//', 'eht///////', 'oitanimile', 'fo////////', 'ratirohtua', 'oitutitsni', 'ralucitrap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcrana///', 'sa////////', 'tsom//////', 'stsihcrana', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylpmi/////', 'soahc/////', 'msilihin//', 'ro////////', 'eimona////', 'tub///////', 'rehtar////', 'a/////////', 'suoinomrah', 'itna//////']\n",
      "['//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////']\n",
      "Minibatch perplexity: 780690994398624313498189175424028795405874117923453252520011355400710754784737612915539968.00\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "saving 0\n",
      "Average loss at step 200: 0.948194 learning rate: 5.000000\n",
      "['authoritar', '///society', '////////in', '/////place', '////////of', '//////what', '///////are', '//regarded', '////////as', 'authoritar', '/political', 'structures', '///////and', '//coercive', '//economic', '/instituti', '/anarchism', 'originated', '////////as', '/////////a', '//////term', '////////of', '/////abuse', '/////first', '//////used', '///against', '/////early', '///working', '/////class', '//radicals', '/including', '///////the', '///diggers', '////////of', '///////the', '///english', 'revolution', '///////and', '///////the', '//////sans', '//culottes', '////////of', '///////the', '////french', 'revolution', '////whilst', '///////the', '//////term', '////////is', '/////still', '//////used', '////////in', '/////////a', 'pejorative', '///////way', '////////to', '//describe', '///////any', '///////act', '//////that', '//////used', '///violent', '/////means', '////////to', '///destroy', '///////the', 'organizati', '////////of', '///society', '////////it', '///////has', '//////also', '//////been', '/////taken', '////////up', '////////as', '/////////a', '//positive', '/////label', '////////by', '//////self', '///defined', 'anarchists', '///////the', '//////word', '/anarchism', '////////is', '///derived', '//////from', '///////the', '/////greek', '///without', '///archons', '/////ruler', '/////chief', '//////king', '/anarchism', '////////as', '/////////a', '/political']\n",
      "['ratirohtua', 'yteicos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'dedrager//', 'sa////////', 'ratirohtua', 'lacitilop/', 'serutcurts', 'dna///////', 'evicreoc//', 'cimonoce//', 'itutitsni/', 'msihcrana/', 'detanigiro', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///', 'ylrae/////', 'gnikrow///', 'ssalc/////', 'slacidar//', 'gnidulcni/', 'eht///////', 'sreggid///', 'fo////////', 'eht///////', 'hsilgne///', 'noitulover', 'dna///////', 'eht///////', 'snas//////', 'settoluc//', 'fo////////', 'eht///////', 'hcnerf////', 'noitulover', 'tslihw////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////', 'a/////////', 'evitarojep', 'yaw///////', 'ot////////', 'ebircsed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tneloiv///', 'snaem/////', 'ot////////', 'yortsed///', 'eht///////', 'itazinagro', 'fo////////', 'yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evitisop//', 'lebal/////', 'yb////////', 'fles//////', 'denifed///', 'stsihcrana', 'eht///////', 'drow//////', 'msihcrana/', 'si////////', 'devired///', 'morf//////', 'eht///////', 'keerg/////', 'tuohtiw///', 'snohcra///', 'relur/////', 'feihc/////', 'gnik//////', 'msihcrana/', 'sa////////', 'a/////////', 'lacitilop/']\n",
      "['rohshhhtao', 'ytemdis///', 'ni////////', 'ecapp/////', 'fo////////', 'taht//////', 'era///////', 'deraal/r//', 'sa////////', 'rohshhhtao', 'lirirsrlr/', 'serotsetts', 'dna///////', 'erif/uff//', 'cnnmcnc///', 'ntaasntsut', 'isihcaa/a/', 'detnnic/de', 'sa////////', 'a/////////', 'eret//////', 'fo////////', 'esuaa/////', 'tsiit/////', 'desu//////', 'tsiatm////', 'yllael////', 'gnir/of///', 'asawl/////', 'slicirara/', 'gnicoouni/', 'eht///////', 'seeiiid///', 'fo////////', 'eht///////', 'ggseeeeh//', 'noitaolrer', 'dna///////', 'eht///////', 'saa///////', 'setsstcc//', 'fo////////', 'eht///////', 'cceeere///', 'noitaolrer', 'tsiiht////', 'eht///////', 'eret//////', 'si////////', 'ilits/////', 'desu//////', 'ni////////', 'a/////////', 'etitrrpp/r', 'yaw///////', 'ot////////', 'ecise//r//', 'yaa///////', 'tca///////', 'taht//////', 'desu//////', 'tnevlile//', 'snaem/////', 'ot////////', 'yyrtset///', 'eht///////', 'itar/nir/o', 'fo////////', 'ytemdis///', 'ti////////', 'sah///////', 'ossa//////', 'neeb//////', 'net/t/////', '//////////', 'sa////////', 'a/////////', 'evitosst//', 'lala//////', 'yb////////', 'esel//////', 'decic/n///', 'issihcaa/a', 'eht///////', 'ddod//////', 'isihcaa/a/', 'si////////', 'devirpv///', 'morf//////', 'eht///////', 'keerg/////', 'tsohtih///', 'soohcaa///', 'repulu////', 'foifc/////', 'gnig//////', 'isihcaa/a/', 'sa////////', 'a/////////', 'lipipsspi/']\n",
      "Minibatch perplexity: 467837407604375236387012608.00\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "saving 200\n",
      "Average loss at step 400: 0.463609 learning rate: 5.000000\n",
      "['philosophy', '////////is', '///////the', '////belief', '//////that', '////rulers', '///////are', 'unnecessar', '///////and', '////should', '////////be', '/abolished', '//although', '/////there', '///////are', '/differing', 'interpreta', '////////of', '//////what', '//////this', '/////means', '/anarchism', '//////also', '////refers', '////////to', '///related', '////social', '/movements', '//////that', '//advocate', '///////the', 'eliminatio', '////////of', 'authoritar', 'institutio', 'particular', '///////the', '/////state', '///////the', '//////word', '///anarchy', '////////as', '//////most', 'anarchists', '///////use', '////////it', '//////does', '///////not', '/////imply', '/////chaos', '//nihilism', '////////or', '////anomie', '///////but', '////rather', '/////////a', 'harmonious', '//////anti', 'authoritar', '///society', '////////in', '/////place', '////////of', '//////what', '///////are', '//regarded', '////////as', 'authoritar', '/political', 'structures', '///////and', '//coercive', '//economic', '/instituti', '/anarchism', 'originated', '////////as', '/////////a', '//////term', '////////of', '/////abuse', '/////first', '//////used', '///against', '/////early', '///working', '/////class', '//radicals', '/including', '///////the', '///diggers', '////////of', '///////the', '///english', 'revolution', '///////and', '///////the', '//////sans', '//culottes', '////////of']\n",
      "['yhposolihp', 'si////////', 'eht///////', 'feileb////', 'taht//////', 'srelur////', 'era///////', 'rassecennu', 'dna///////', 'dluohs////', 'eb////////', 'dehsiloba/', 'hguohtla//', 'ereht/////', 'era///////', 'gnireffid/', 'aterpretni', 'fo////////', 'tahw//////', 'siht//////', 'snaem/////', 'msihcrana/', 'osla//////', 'srefer////', 'ot////////', 'detaler///', 'laicos////', 'stnemevom/', 'taht//////', 'etacovda//', 'eht///////', 'oitanimile', 'fo////////', 'ratirohtua', 'oitutitsni', 'ralucitrap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcrana///', 'sa////////', 'tsom//////', 'stsihcrana', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylpmi/////', 'soahc/////', 'msilihin//', 'ro////////', 'eimona////', 'tub///////', 'rehtar////', 'a/////////', 'suoinomrah', 'itna//////', 'ratirohtua', 'yteicos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'dedrager//', 'sa////////', 'ratirohtua', 'lacitilop/', 'serutcurts', 'dna///////', 'evicreoc//', 'cimonoce//', 'itutitsni/', 'msihcrana/', 'detanigiro', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///', 'ylrae/////', 'gnikrow///', 'ssalc/////', 'slacidar//', 'gnidulcni/', 'eht///////', 'sreggid///', 'fo////////', 'eht///////', 'hsilgne///', 'noitulover', 'dna///////', 'eht///////', 'snas//////', 'settoluc//', 'fo////////']\n",
      "['yhpasohahp', 'si////////', 'eht///////', 'ieided////', 'taht//////', 'srevu/u///', 'era///////', 'eeceersree', 'dna///////', 'dluohp////', 'eb////////', 'dehailah//', 'gguohtia//', 'ereht/////', 'era///////', 'gnidedied/', 'nnnne/enni', 'fo////////', 'tahw//////', 'siht//////', 'seamma////', 'ooihcaa/a/', 'osya//////', 'seefere///', 'ot////////', 'detarer///', 'aaicis////', 'ntnemevoce', 'taht//////', 'etaca/////', 'eht///////', 'nntamemene', 'fo////////', 'rotihoh/aa', 'tttcrnnser', 'totucitsa/', 'eht///////', 'etatse////', 'eht///////', 'drow//////', 'yhcaanam//', 'sa////////', 'tsom//////', 'iccnrcaa/a', 'esus//////', 'ti////////', 'sood//////', 'ton///////', 'ylimi/////', 'aahhca////', 'ilihih/ln/', 'ro////////', 'nnnona////', 'tub///////', 'retta/a///', 'a/////////', 'nnohrahaah', 'itaa//////', 'rotihoh/aa', 'ytebdas///', 'ni////////', 'ecapp/////', 'fo////////', 'tahw//////', 'era///////', 'deraat/r//', 'sa////////', 'rotihoh/aa', 'lacitatpt/', 'serctcos/s', 'dna///////', 'eviceevd//', 'cinenec/c/', 'itaasnsnit', 'ooihcaa/a/', 'detanicoco', 'sa////////', 'a/////////', 'eret//////', 'fo////////', 'esaaa/////', 'tiiif/////', 'desus/////', 'tnnirga///', 'ylaaem////', 'gniloow///', 'allcc/////', 'slacirara/', 'gniduccni/', 'eht///////', 'seebiid///', 'fo////////', 'eht///////', 'ssnhenes//', 'noitugerir', 'dna///////', 'eht///////', 'saa///////', 'selaaccca/', 'fo////////']\n",
      "Minibatch perplexity: 116535529621778392785757405184.00\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "Average loss at step 600: 0.362100 learning rate: 4.800000\n",
      "['///////the', '////french', 'revolution', '////whilst', '///////the', '//////term', '////////is', '/////still', '//////used', '////////in', '/////////a', 'pejorative', '///////way', '////////to', '//describe', '///////any', '///////act', '//////that', '//////used', '///violent', '/////means', '////////to', '///destroy', '///////the', 'organizati', '////////of', '///society', '////////it', '///////has', '//////also', '//////been', '/////taken', '////////up', '////////as', '/////////a', '//positive', '/////label', '////////by', '//////self', '///defined', 'anarchists', '///////the', '//////word', '/anarchism', '////////is', '///derived', '//////from', '///////the', '/////greek', '///without', '///archons', '/////ruler', '/////chief', '//////king', '/anarchism', '////////as', '/////////a', '/political', 'philosophy', '////////is', '///////the', '////belief', '//////that', '////rulers', '///////are', 'unnecessar', '///////and', '////should', '////////be', '/abolished', '//although', '/////there', '///////are', '/differing', 'interpreta', '////////of', '//////what', '//////this', '/////means', '/anarchism', '//////also', '////refers', '////////to', '///related', '////social', '/movements', '//////that', '//advocate', '///////the', 'eliminatio', '////////of', 'authoritar', 'institutio', 'particular', '///////the', '/////state', '///////the', '//////word', '///anarchy', '////////as']\n",
      "['eht///////', 'hcnerf////', 'noitulover', 'tslihw////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////', 'a/////////', 'evitarojep', 'yaw///////', 'ot////////', 'ebircsed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tneloiv///', 'snaem/////', 'ot////////', 'yortsed///', 'eht///////', 'itazinagro', 'fo////////', 'yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evitisop//', 'lebal/////', 'yb////////', 'fles//////', 'denifed///', 'stsihcrana', 'eht///////', 'drow//////', 'msihcrana/', 'si////////', 'devired///', 'morf//////', 'eht///////', 'keerg/////', 'tuohtiw///', 'snohcra///', 'relur/////', 'feihc/////', 'gnik//////', 'msihcrana/', 'sa////////', 'a/////////', 'lacitilop/', 'yhposolihp', 'si////////', 'eht///////', 'feileb////', 'taht//////', 'srelur////', 'era///////', 'rassecennu', 'dna///////', 'dluohs////', 'eb////////', 'dehsiloba/', 'hguohtla//', 'ereht/////', 'era///////', 'gnireffid/', 'aterpretni', 'fo////////', 'tahw//////', 'siht//////', 'snaem/////', 'msihcrana/', 'osla//////', 'srefer////', 'ot////////', 'detaler///', 'laicos////', 'stnemevom/', 'taht//////', 'etacovda//', 'eht///////', 'oitanimile', 'fo////////', 'ratirohtua', 'oitutitsni', 'ralucitrap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcrana///', 'sa////////']\n",
      "['eht///////', 'hcnerf////', 'noituoorrr', 'tsliht////', 'eht///////', 'eret//////', 'si////////', 'itits/////', 'desu//////', 'ni////////', 'a/////////', 'evitorpppp', 'yaw///////', 'ot////////', 'edirosed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tnevlivi//', 'snamm/////', 'ot////////', 'yortsed///', 'eht///////', 'itaginor/o', 'fo////////', 'ytiisos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'netae/////', 'pu////////', 'sa////////', 'a/////////', 'evitosoc//', 'lelal/////', 'yb////////', 'fles//////', 'denifed///', 'stnircn///', 'eht///////', 'drow//////', 'msimcnanam', 'si////////', 'devided///', 'morf//////', 'eht///////', 'reerg/////', 'tuohtiw///', 'snohcaa///', 'redul/////', 'fiifc/////', 'gnig//////', 'msimcnanam', 'sa////////', 'a/////////', 'latitilop/', 'yhposolihp', 'si////////', 'eht///////', 'ililed////', 'taht//////', 'sredur////', 'era///////', 'cuusurnruu', 'dna///////', 'dluohs////', 'eb////////', 'dehsilohat', 'hguomtaa//', 'ereht/////', 'era///////', 'gnided/ed/', 'hhnrpmopni', 'fo////////', 'tahw//////', 'siht//////', 'snamm/////', 'msimcnanam', 'osla//////', 'srefer////', 'ot////////', 'detar/r///', 'laicis////', 'stnemucoc/', 'taht//////', 'etacuc////', 'eht///////', 'oitanelole', 'fo////////', 'ratnerhpeo', 'ostsdnvsee', 'ratucirsap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yrrnanar//', 'sa////////']\n",
      "Minibatch perplexity: 5401239416783959.00\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "saving 600\n",
      "Average loss at step 800: 0.309878 learning rate: 4.800000\n",
      "['//////most', 'anarchists', '///////use', '////////it', '//////does', '///////not', '/////imply', '/////chaos', '//nihilism', '////////or', '////anomie', '///////but', '////rather', '/////////a', 'harmonious', '//////anti', 'authoritar', '///society', '////////in', '/////place', '////////of', '//////what', '///////are', '//regarded', '////////as', 'authoritar', '/political', 'structures', '///////and', '//coercive', '//economic', '/instituti', '/anarchism', 'originated', '////////as', '/////////a', '//////term', '////////of', '/////abuse', '/////first', '//////used', '///against', '/////early', '///working', '/////class', '//radicals', '/including', '///////the', '///diggers', '////////of', '///////the', '///english', 'revolution', '///////and', '///////the', '//////sans', '//culottes', '////////of', '///////the', '////french', 'revolution', '////whilst', '///////the', '//////term', '////////is', '/////still', '//////used', '////////in', '/////////a', 'pejorative', '///////way', '////////to', '//describe', '///////any', '///////act', '//////that', '//////used', '///violent', '/////means', '////////to', '///destroy', '///////the', 'organizati', '////////of', '///society', '////////it', '///////has', '//////also', '//////been', '/////taken', '////////up', '////////as', '/////////a', '//positive', '/////label', '////////by', '//////self', '///defined', 'anarchists', '///////the']\n",
      "['tsom//////', 'stsihcrana', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylpmi/////', 'soahc/////', 'msilihin//', 'ro////////', 'eimona////', 'tub///////', 'rehtar////', 'a/////////', 'suoinomrah', 'itna//////', 'ratirohtua', 'yteicos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'dedrager//', 'sa////////', 'ratirohtua', 'lacitilop/', 'serutcurts', 'dna///////', 'evicreoc//', 'cimonoce//', 'itutitsni/', 'msihcrana/', 'detanigiro', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///', 'ylrae/////', 'gnikrow///', 'ssalc/////', 'slacidar//', 'gnidulcni/', 'eht///////', 'sreggid///', 'fo////////', 'eht///////', 'hsilgne///', 'noitulover', 'dna///////', 'eht///////', 'snas//////', 'settoluc//', 'fo////////', 'eht///////', 'hcnerf////', 'noitulover', 'tslihw////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////', 'a/////////', 'evitarojep', 'yaw///////', 'ot////////', 'ebircsed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tneloiv///', 'snaem/////', 'ot////////', 'yortsed///', 'eht///////', 'itazinagro', 'fo////////', 'yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evitisop//', 'lebal/////', 'yb////////', 'fles//////', 'denifed///', 'stsihcrana', 'eht///////']\n",
      "['tsom//////', 'stsircnana', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylimi/////', 'saohca////', 'isslih/mn/', 'ro////////', 'mmmmnni///', 'tub///////', 'rehtar////', 'a/////////', 'suorrumooh', 'itni//////', 'ratihrhtaa', 'ytiicos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'aerral////', 'sa////////', 'ratihrhtaa', 'lacitispp/', 'serutcerts', 'dna///////', 'evirrudr//', 'cimenece//', 'ititins/i/', 'msimcramam', 'detanigicc', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///', 'ylrael////', 'gniwrow///', 'asall/////', 'slacirrrt/', 'gnidulcni/', 'eht///////', 'seediid///', 'fo////////', 'eht///////', 'ssnllnes//', 'noitugorer', 'dna///////', 'eht///////', 'sna///////', 'satlacac//', 'fo////////', 'eht///////', 'ccnerf////', 'noitugorer', 'tsliht////', 'eht///////', 'mret//////', 'si////////', 'itits/////', 'desu//////', 'ni////////', 'a/////////', 'evitaloppp', 'yaw///////', 'ot////////', 'egircs/b//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tnevlivi//', 'snaem/////', 'ot////////', 'yortsed///', 'eht///////', 'itagigogoo', 'fo////////', 'ytiicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nek/t/////', 'pu////////', 'sa////////', 'a/////////', 'evitosopm/', 'lebalb////', 'yb////////', 'lles//////', 'denifed///', 'stsircnana', 'eht///////']\n",
      "Minibatch perplexity: 147944497369038.66\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "saving 800\n",
      "Average loss at step 1000: 0.294646 learning rate: 4.608000\n",
      "['//////word', '/anarchism', '////////is', '///derived', '//////from', '///////the', '/////greek', '///without', '///archons', '/////ruler', '/////chief', '//////king', '/anarchism', '////////as', '/////////a', '/political', 'philosophy', '////////is', '///////the', '////belief', '//////that', '////rulers', '///////are', 'unnecessar', '///////and', '////should', '////////be', '/abolished', '//although', '/////there', '///////are', '/differing', 'interpreta', '////////of', '//////what', '//////this', '/////means', '/anarchism', '//////also', '////refers', '////////to', '///related', '////social', '/movements', '//////that', '//advocate', '///////the', 'eliminatio', '////////of', 'authoritar', 'institutio', 'particular', '///////the', '/////state', '///////the', '//////word', '///anarchy', '////////as', '//////most', 'anarchists', '///////use', '////////it', '//////does', '///////not', '/////imply', '/////chaos', '//nihilism', '////////or', '////anomie', '///////but', '////rather', '/////////a', 'harmonious', '//////anti', 'authoritar', '///society', '////////in', '/////place', '////////of', '//////what', '///////are', '//regarded', '////////as', 'authoritar', '/political', 'structures', '///////and', '//coercive', '//economic', '/instituti', '/anarchism', 'originated', '////////as', '/////////a', '//////term', '////////of', '/////abuse', '/////first', '//////used', '///against']\n",
      "['drow//////', 'msihcrana/', 'si////////', 'devired///', 'morf//////', 'eht///////', 'keerg/////', 'tuohtiw///', 'snohcra///', 'relur/////', 'feihc/////', 'gnik//////', 'msihcrana/', 'sa////////', 'a/////////', 'lacitilop/', 'yhposolihp', 'si////////', 'eht///////', 'feileb////', 'taht//////', 'srelur////', 'era///////', 'rassecennu', 'dna///////', 'dluohs////', 'eb////////', 'dehsiloba/', 'hguohtla//', 'ereht/////', 'era///////', 'gnireffid/', 'aterpretni', 'fo////////', 'tahw//////', 'siht//////', 'snaem/////', 'msihcrana/', 'osla//////', 'srefer////', 'ot////////', 'detaler///', 'laicos////', 'stnemevom/', 'taht//////', 'etacovda//', 'eht///////', 'oitanimile', 'fo////////', 'ratirohtua', 'oitutitsni', 'ralucitrap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcrana///', 'sa////////', 'tsom//////', 'stsihcrana', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylpmi/////', 'soahc/////', 'msilihin//', 'ro////////', 'eimona////', 'tub///////', 'rehtar////', 'a/////////', 'suoinomrah', 'itna//////', 'ratirohtua', 'yteicos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'dedrager//', 'sa////////', 'ratirohtua', 'lacitilop/', 'serutcurts', 'dna///////', 'evicreoc//', 'cimonoce//', 'itutitsni/', 'msihcrana/', 'detanigiro', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///']\n",
      "['drow//////', 'msimcaamam', 'si////////', 'devided///', 'morf//////', 'eht///////', 'keerg/////', 'tuohti////', 'snohcra///', 'reluu/////', 'feihc/////', 'gnik//////', 'msnmcaamam', 'sa////////', 'a/////////', 'lacitisop/', 'yhposibihp', 'si////////', 'eht///////', 'feiled////', 'taht//////', 'srelul////', 'era///////', 'rucuucrrnu', 'dna///////', 'dluohs////', 'eb////////', 'dehsiloha/', 'hguohtaa//', 'ereht/////', 'era///////', 'gnifef/id/', 'atnre/etni', 'fo////////', 'tahw//////', 'siht//////', 'snamm/////', 'msimcaamam', 'osla//////', 'srefer////', 'ot////////', 'detalere//', 'laicos////', 'stnemuvome', 'taht//////', 'etacuv/a//', 'eht///////', 'oitanemole', 'fo////////', 'ratihhhtua', 'oitutissee', 'raluoirsom', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yrcna/////', 'sa////////', 'tsom//////', 'stsihrra//', 'esu///////', 'ti////////', 'sdodo/////', 'ton///////', 'ylimi/////', 'sohhc/////', 'issnih/mi/', 'ro////////', 'eimoni////', 'tub///////', 'rehtar////', 'a/////////', 'suohhahoom', 'itni//////', 'ratirohpro', 'ytiscos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'degaag/b//', 'sa////////', 'ratihhhtua', 'lacitispp/', 'serutcerts', 'dna///////', 'eviciofr//', 'cimococ///', 'itutntsnoe', 'msimcaamam', 'deganicido', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tnniag////']\n",
      "Minibatch perplexity: 5679484352613164.00\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "Average loss at step 1200: 0.227260 learning rate: 4.608000\n",
      "['/////early', '///working', '/////class', '//radicals', '/including', '///////the', '///diggers', '////////of', '///////the', '///english', 'revolution', '///////and', '///////the', '//////sans', '//culottes', '////////of', '///////the', '////french', 'revolution', '////whilst', '///////the', '//////term', '////////is', '/////still', '//////used', '////////in', '/////////a', 'pejorative', '///////way', '////////to', '//describe', '///////any', '///////act', '//////that', '//////used', '///violent', '/////means', '////////to', '///destroy', '///////the', 'organizati', '////////of', '///society', '////////it', '///////has', '//////also', '//////been', '/////taken', '////////up', '////////as', '/////////a', '//positive', '/////label', '////////by', '//////self', '///defined', 'anarchists', '///////the', '//////word', '/anarchism', '////////is', '///derived', '//////from', '///////the', '/////greek', '///without', '///archons', '/////ruler', '/////chief', '//////king', '/anarchism', '////////as', '/////////a', '/political', 'philosophy', '////////is', '///////the', '////belief', '//////that', '////rulers', '///////are', 'unnecessar', '///////and', '////should', '////////be', '/abolished', '//although', '/////there', '///////are', '/differing', 'interpreta', '////////of', '//////what', '//////this', '/////means', '/anarchism', '//////also', '////refers', '////////to', '///related']\n",
      "['ylrae/////', 'gnikrow///', 'ssalc/////', 'slacidar//', 'gnidulcni/', 'eht///////', 'sreggid///', 'fo////////', 'eht///////', 'hsilgne///', 'noitulover', 'dna///////', 'eht///////', 'snas//////', 'settoluc//', 'fo////////', 'eht///////', 'hcnerf////', 'noitulover', 'tslihw////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////', 'a/////////', 'evitarojep', 'yaw///////', 'ot////////', 'ebircsed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tneloiv///', 'snaem/////', 'ot////////', 'yortsed///', 'eht///////', 'itazinagro', 'fo////////', 'yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evitisop//', 'lebal/////', 'yb////////', 'fles//////', 'denifed///', 'stsihcrana', 'eht///////', 'drow//////', 'msihcrana/', 'si////////', 'devired///', 'morf//////', 'eht///////', 'keerg/////', 'tuohtiw///', 'snohcra///', 'relur/////', 'feihc/////', 'gnik//////', 'msihcrana/', 'sa////////', 'a/////////', 'lacitilop/', 'yhposolihp', 'si////////', 'eht///////', 'feileb////', 'taht//////', 'srelur////', 'era///////', 'rassecennu', 'dna///////', 'dluohs////', 'eb////////', 'dehsiloba/', 'hguohtla//', 'ereht/////', 'era///////', 'gnireffid/', 'aterpretni', 'fo////////', 'tahw//////', 'siht//////', 'snaem/////', 'msihcrana/', 'osla//////', 'srefer////', 'ot////////', 'detaler///']\n",
      "['ylrae/////', 'gnikoow///', 'asacc/////', 'slacidarg/', 'gnidugcni/', 'eht///////', 'sredddd///', 'fo////////', 'eht///////', 'hsilbnes//', 'noitugover', 'dna///////', 'eht///////', 'sna///////', 'setooltt//', 'fo////////', 'eht///////', 'hcnerf////', 'noitugover', 'tsliht////', 'eht///////', 'mrem//////', 'si////////', 'llitsa////', 'desu//////', 'ni////////', 'a/////////', 'evitaeeppp', 'yaw///////', 'ot////////', 'esiscsec//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tnevlivi//', 'snamm/////', 'ot////////', 'yrrtsed///', 'eht///////', 'itaiinagio', 'fo////////', 'ytestos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekan/////', 'pu////////', 'sa////////', 'a/////////', 'evitepeps/', 'lebal/////', 'yb////////', 'fles//////', 'denifed///', 'stsincna//', 'eht///////', 'drow//////', 'msimrnamam', 'si////////', 'devided///', 'morf//////', 'eht///////', 'keerg/////', 'tuhhtiw///', 'snohcra///', 'reluu/////', 'feefc/////', 'gnik//////', 'msimrnamam', 'sa////////', 'a/////////', 'lacitippp/', 'yhpoiilihp', 'si////////', 'eht///////', 'feilbb////', 'taht//////', 'srelup////', 'era///////', 'reuuuc/rnu', 'dna///////', 'dluohs////', 'eb////////', 'dehoiladaa', 'hguohtla//', 'ereht/////', 'era///////', 'gnidediid/', 'apsrpmepni', 'fo////////', 'tahw//////', 'siht//////', 'snamm/////', 'msihrnamam', 'osla//////', 'srefer////', 'ot////////', 'detaler///']\n",
      "Minibatch perplexity: 461701435807.94\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "saving 1200\n",
      "Average loss at step 1400: 0.223844 learning rate: 4.608000\n",
      "['////social', '/movements', '//////that', '//advocate', '///////the', 'eliminatio', '////////of', 'authoritar', 'institutio', 'particular', '///////the', '/////state', '///////the', '//////word', '///anarchy', '////////as', '//////most', 'anarchists', '///////use', '////////it', '//////does', '///////not', '/////imply', '/////chaos', '//nihilism', '////////or', '////anomie', '///////but', '////rather', '/////////a', 'harmonious', '//////anti', 'authoritar', '///society', '////////in', '/////place', '////////of', '//////what', '///////are', '//regarded', '////////as', 'authoritar', '/political', 'structures', '///////and', '//coercive', '//economic', '/instituti', '/anarchism', 'originated', '////////as', '/////////a', '//////term', '////////of', '/////abuse', '/////first', '//////used', '///against', '/////early', '///working', '/////class', '//radicals', '/including', '///////the', '///diggers', '////////of', '///////the', '///english', 'revolution', '///////and', '///////the', '//////sans', '//culottes', '////////of', '///////the', '////french', 'revolution', '////whilst', '///////the', '//////term', '////////is', '/////still', '//////used', '////////in', '/////////a', 'pejorative', '///////way', '////////to', '//describe', '///////any', '///////act', '//////that', '//////used', '///violent', '/////means', '////////to', '///destroy', '///////the', 'organizati', '////////of']\n",
      "['laicos////', 'stnemevom/', 'taht//////', 'etacovda//', 'eht///////', 'oitanimile', 'fo////////', 'ratirohtua', 'oitutitsni', 'ralucitrap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcrana///', 'sa////////', 'tsom//////', 'stsihcrana', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylpmi/////', 'soahc/////', 'msilihin//', 'ro////////', 'eimona////', 'tub///////', 'rehtar////', 'a/////////', 'suoinomrah', 'itna//////', 'ratirohtua', 'yteicos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'dedrager//', 'sa////////', 'ratirohtua', 'lacitilop/', 'serutcurts', 'dna///////', 'evicreoc//', 'cimonoce//', 'itutitsni/', 'msihcrana/', 'detanigiro', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///', 'ylrae/////', 'gnikrow///', 'ssalc/////', 'slacidar//', 'gnidulcni/', 'eht///////', 'sreggid///', 'fo////////', 'eht///////', 'hsilgne///', 'noitulover', 'dna///////', 'eht///////', 'snas//////', 'settoluc//', 'fo////////', 'eht///////', 'hcnerf////', 'noitulover', 'tslihw////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////', 'a/////////', 'evitarojep', 'yaw///////', 'ot////////', 'ebircsed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tneloiv///', 'snaem/////', 'ot////////', 'yortsed///', 'eht///////', 'itazinagro', 'fo////////']\n",
      "['laicis////', 'stnemuvom/', 'taht//////', 'etauuv////', 'eht///////', 'oitanemele', 'fo////////', 'ratihrh/ua', 'oituostsii', 'rapupirsap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yncnana///', 'sa////////', 'tsom//////', 'stsihcra/a', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'yllmil////', 'sohhc/////', 'isilih/mi/', 'ro////////', 'eemmnn////', 'tub///////', 'rehtar////', 'a/////////', 'suommrmrah', 'itaa//////', 'ratiorh/ua', 'ytessos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'derrae/r//', 'sa////////', 'ratihrh/ua', 'lacitilop/', 'serutcus/s', 'dna///////', 'evirromr//', 'cimococe//', 'itststs/u/', 'msimcnamim', 'detanigrro', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///', 'ylrae/////', 'gniwrow///', 'ssalc/////', 'slacirrrc/', 'gnidulcni/', 'eht///////', 'sreddid///', 'fo////////', 'eht///////', 'hsiline///', 'noitugover', 'dna///////', 'eht///////', 'snas//////', 'setlllut//', 'fo////////', 'eht///////', 'hcnerf////', 'noitugover', 'tsiihw////', 'eht///////', 'mret//////', 'si////////', 'ilits/////', 'desu//////', 'ni////////', 'a/////////', 'evitaloppp', 'yaw///////', 'ot////////', 'ebirceed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tnevlvv///', 'snaem/////', 'ot////////', 'yottsed///', 'eht///////', 'itaginogro', 'fo////////']\n",
      "Minibatch perplexity: 1133482098515.03\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "Average loss at step 1600: 0.230816 learning rate: 4.423680\n",
      "['///society', '////////it', '///////has', '//////also', '//////been', '/////taken', '////////up', '////////as', '/////////a', '//positive', '/////label', '////////by', '//////self', '///defined', 'anarchists', '///////the', '//////word', '/anarchism', '////////is', '///derived', '//////from', '///////the', '/////greek', '///without', '///archons', '/////ruler', '/////chief', '//////king', '/anarchism', '////////as', '/////////a', '/political', 'philosophy', '////////is', '///////the', '////belief', '//////that', '////rulers', '///////are', 'unnecessar', '///////and', '////should', '////////be', '/abolished', '//although', '/////there', '///////are', '/differing', 'interpreta', '////////of', '//////what', '//////this', '/////means', '/anarchism', '//////also', '////refers', '////////to', '///related', '////social', '/movements', '//////that', '//advocate', '///////the', 'eliminatio', '////////of', 'authoritar', 'institutio', 'particular', '///////the', '/////state', '///////the', '//////word', '///anarchy', '////////as', '//////most', 'anarchists', '///////use', '////////it', '//////does', '///////not', '/////imply', '/////chaos', '//nihilism', '////////or', '////anomie', '///////but', '////rather', '/////////a', 'harmonious', '//////anti', 'authoritar', '///society', '////////in', '/////place', '////////of', '//////what', '///////are', '//regarded', '////////as', 'authoritar']\n",
      "['yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evitisop//', 'lebal/////', 'yb////////', 'fles//////', 'denifed///', 'stsihcrana', 'eht///////', 'drow//////', 'msihcrana/', 'si////////', 'devired///', 'morf//////', 'eht///////', 'keerg/////', 'tuohtiw///', 'snohcra///', 'relur/////', 'feihc/////', 'gnik//////', 'msihcrana/', 'sa////////', 'a/////////', 'lacitilop/', 'yhposolihp', 'si////////', 'eht///////', 'feileb////', 'taht//////', 'srelur////', 'era///////', 'rassecennu', 'dna///////', 'dluohs////', 'eb////////', 'dehsiloba/', 'hguohtla//', 'ereht/////', 'era///////', 'gnireffid/', 'aterpretni', 'fo////////', 'tahw//////', 'siht//////', 'snaem/////', 'msihcrana/', 'osla//////', 'srefer////', 'ot////////', 'detaler///', 'laicos////', 'stnemevom/', 'taht//////', 'etacovda//', 'eht///////', 'oitanimile', 'fo////////', 'ratirohtua', 'oitutitsni', 'ralucitrap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcrana///', 'sa////////', 'tsom//////', 'stsihcrana', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylpmi/////', 'soahc/////', 'msilihin//', 'ro////////', 'eimona////', 'tub///////', 'rehtar////', 'a/////////', 'suoinomrah', 'itna//////', 'ratirohtua', 'yteicos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'dedrager//', 'sa////////', 'ratirohtua']\n",
      "['yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evisosopp/', 'lebal/////', 'yb////////', 'fles//////', 'denided///', 'stsircra/a', 'eht///////', 'drow//////', 'msincrama/', 'si////////', 'devired///', 'morf//////', 'eht///////', 'keerg/////', 'tuohtiw///', 'snohcra///', 'reluu/////', 'feefci////', 'gnik//////', 'msincrama/', 'sa////////', 'a/////////', 'lacitidip/', 'yhposilihc', 'si////////', 'eht///////', 'feilib////', 'taht//////', 'srelup////', 'era///////', 'rucuucuuuu', 'dna///////', 'dluohs////', 'eb////////', 'dehsidabal', 'hguohtla//', 'ereht/////', 'era///////', 'gniffffid/', 'atnrpnetni', 'fo////////', 'tahw//////', 'siht//////', 'snaem/////', 'msincrama/', 'osla//////', 'srefer////', 'ot////////', 'detaler///', 'laicis////', 'stnemuvem/', 'taht//////', 'etacucea//', 'eht///////', 'oitanelele', 'fo////////', 'ratihoh/ua', 'oitufissou', 'ralucitsap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcnana///', 'sa////////', 'tsom//////', 'stsircra//', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylpmi/////', 'sohhc/////', 'isilih/ni/', 'ro////////', 'eimonn////', 'tub///////', 'rehtar////', 'a/////////', 'suomnrrrah', 'itna//////', 'ratihoh/ua', 'yteicos///', 'ni////////', 'ecapp/////', 'fo////////', 'tahw//////', 'era///////', 'derrae/r//', 'sa////////', 'ratihoh/ua']\n",
      "Minibatch perplexity: 238938761696.10\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "saving 1600\n",
      "Average loss at step 1800: 0.189047 learning rate: 4.423680\n",
      "['/political', 'structures', '///////and', '//coercive', '//economic', '/instituti', '/anarchism', 'originated', '////////as', '/////////a', '//////term', '////////of', '/////abuse', '/////first', '//////used', '///against', '/////early', '///working', '/////class', '//radicals', '/including', '///////the', '///diggers', '////////of', '///////the', '///english', 'revolution', '///////and', '///////the', '//////sans', '//culottes', '////////of', '///////the', '////french', 'revolution', '////whilst', '///////the', '//////term', '////////is', '/////still', '//////used', '////////in', '/////////a', 'pejorative', '///////way', '////////to', '//describe', '///////any', '///////act', '//////that', '//////used', '///violent', '/////means', '////////to', '///destroy', '///////the', 'organizati', '////////of', '///society', '////////it', '///////has', '//////also', '//////been', '/////taken', '////////up', '////////as', '/////////a', '//positive', '/////label', '////////by', '//////self', '///defined', 'anarchists', '///////the', '//////word', '/anarchism', '////////is', '///derived', '//////from', '///////the', '/////greek', '///without', '///archons', '/////ruler', '/////chief', '//////king', '/anarchism', '////////as', '/////////a', '/political', 'philosophy', '////////is', '///////the', '////belief', '//////that', '////rulers', '///////are', 'unnecessar', '///////and', '////should']\n",
      "['lacitilop/', 'serutcurts', 'dna///////', 'evicreoc//', 'cimonoce//', 'itutitsni/', 'msihcrana/', 'detanigiro', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///', 'ylrae/////', 'gnikrow///', 'ssalc/////', 'slacidar//', 'gnidulcni/', 'eht///////', 'sreggid///', 'fo////////', 'eht///////', 'hsilgne///', 'noitulover', 'dna///////', 'eht///////', 'snas//////', 'settoluc//', 'fo////////', 'eht///////', 'hcnerf////', 'noitulover', 'tslihw////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////', 'a/////////', 'evitarojep', 'yaw///////', 'ot////////', 'ebircsed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tneloiv///', 'snaem/////', 'ot////////', 'yortsed///', 'eht///////', 'itazinagro', 'fo////////', 'yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evitisop//', 'lebal/////', 'yb////////', 'fles//////', 'denifed///', 'stsihcrana', 'eht///////', 'drow//////', 'msihcrana/', 'si////////', 'devired///', 'morf//////', 'eht///////', 'keerg/////', 'tuohtiw///', 'snohcra///', 'relur/////', 'feihc/////', 'gnik//////', 'msihcrana/', 'sa////////', 'a/////////', 'lacitilop/', 'yhposolihp', 'si////////', 'eht///////', 'feileb////', 'taht//////', 'srelur////', 'era///////', 'rassecennu', 'dna///////', 'dluohs////']\n",
      "['lacitilpp/', 'secutcerts', 'dna///////', 'evirremr//', 'cinonoce//', 'ttninttsut', 'msimcaama/', 'detanigiro', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniag////', 'ylrae/////', 'gnikrow///', 'asalc/////', 'slacidrr//', 'gnidnlcni/', 'eht///////', 'sregiid///', 'fo////////', 'eht///////', 'hsilgne///', 'noitugorer', 'dna///////', 'eht///////', 'sna///////', 'selclluta/', 'fo////////', 'eht///////', 'hcnerf////', 'noitugorer', 'tsliht////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////', 'a/////////', 'evitarop/p', 'yaw///////', 'ot////////', 'ebircsed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tnelliv///', 'snaem/////', 'ot////////', 'yottsed///', 'eht///////', 'itatinagro', 'fo////////', 'yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evitosops/', 'lebab/////', 'yb////////', 'fles//////', 'denieed///', 'stsirrra//', 'eht///////', 'drow//////', 'msimcrama/', 'si////////', 'devir/d///', 'morf//////', 'eht///////', 'keekg/////', 'tuohtiw///', 'snohcra///', 'relur/////', 'feihc/////', 'gnik//////', 'msimcrama/', 'sa////////', 'a/////////', 'lacitilpp/', 'yhpolilhhp', 'si////////', 'eht///////', 'feilbb////', 'taht//////', 'srelur////', 'era///////', 'ruuuuc/uuu', 'dna///////', 'dluohs////']\n",
      "Minibatch perplexity: 44260480029.79\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "saving 1800\n",
      "Average loss at step 2000: 0.237650 learning rate: 4.246732\n",
      "['////////be', '/abolished', '//although', '/////there', '///////are', '/differing', 'interpreta', '////////of', '//////what', '//////this', '/////means', '/anarchism', '//////also', '////refers', '////////to', '///related', '////social', '/movements', '//////that', '//advocate', '///////the', 'eliminatio', '////////of', 'authoritar', 'institutio', 'particular', '///////the', '/////state', '///////the', '//////word', '///anarchy', '////////as', '//////most', 'anarchists', '///////use', '////////it', '//////does', '///////not', '/////imply', '/////chaos', '//nihilism', '////////or', '////anomie', '///////but', '////rather', '/////////a', 'harmonious', '//////anti', 'authoritar', '///society', '////////in', '/////place', '////////of', '//////what', '///////are', '//regarded', '////////as', 'authoritar', '/political', 'structures', '///////and', '//coercive', '//economic', '/instituti', '/anarchism', 'originated', '////////as', '/////////a', '//////term', '////////of', '/////abuse', '/////first', '//////used', '///against', '/////early', '///working', '/////class', '//radicals', '/including', '///////the', '///diggers', '////////of', '///////the', '///english', 'revolution', '///////and', '///////the', '//////sans', '//culottes', '////////of', '///////the', '////french', 'revolution', '////whilst', '///////the', '//////term', '////////is', '/////still', '//////used', '////////in']\n",
      "['eb////////', 'dehsiloba/', 'hguohtla//', 'ereht/////', 'era///////', 'gnireffid/', 'aterpretni', 'fo////////', 'tahw//////', 'siht//////', 'snaem/////', 'msihcrana/', 'osla//////', 'srefer////', 'ot////////', 'detaler///', 'laicos////', 'stnemevom/', 'taht//////', 'etacovda//', 'eht///////', 'oitanimile', 'fo////////', 'ratirohtua', 'oitutitsni', 'ralucitrap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcrana///', 'sa////////', 'tsom//////', 'stsihcrana', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylpmi/////', 'soahc/////', 'msilihin//', 'ro////////', 'eimona////', 'tub///////', 'rehtar////', 'a/////////', 'suoinomrah', 'itna//////', 'ratirohtua', 'yteicos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'dedrager//', 'sa////////', 'ratirohtua', 'lacitilop/', 'serutcurts', 'dna///////', 'evicreoc//', 'cimonoce//', 'itutitsni/', 'msihcrana/', 'detanigiro', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///', 'ylrae/////', 'gnikrow///', 'ssalc/////', 'slacidar//', 'gnidulcni/', 'eht///////', 'sreggid///', 'fo////////', 'eht///////', 'hsilgne///', 'noitulover', 'dna///////', 'eht///////', 'snas//////', 'settoluc//', 'fo////////', 'eht///////', 'hcnerf////', 'noitulover', 'tslihw////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////']\n",
      "['eb////////', 'dehsilital', 'hguohtla//', 'ereht/////', 'era///////', 'gnifefeid/', 'atsppnepni', 'fo////////', 'tahw//////', 'siht//////', 'snaem/////', 'msimcnama/', 'osla//////', 'srefer////', 'ot////////', 'detaler///', 'laicis////', 'stnemevom/', 'taht//////', 'etaco//a//', 'eht///////', 'oitannmele', 'fo////////', 'ratioohtua', 'oitutsssoo', 'ralucitrap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcnana///', 'sa////////', 'tsom//////', 'stsincra//', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylpmi/////', 'sshhc/////', 'msilid/d//', 'ro////////', 'eimono////', 'tub///////', 'rehtar////', 'a/////////', 'suommamrah', 'itnii/////', 'ratioohtua', 'yteicos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'dekraeeb//', 'sa////////', 'ratioohtua', 'lacitisop/', 'serutcurts', 'dna///////', 'evicrovc//', 'cimonoc///', 'ttutitsnu/', 'msimcnama/', 'detanignco', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///', 'ylrae/////', 'gniwoow///', 'ssalc/////', 'slacidrr//', 'gnidulcni/', 'eht///////', 'sreddid///', 'fo////////', 'eht///////', 'hsilgne///', 'noitugover', 'dna///////', 'eht///////', 'snas//////', 'seutulus//', 'fo////////', 'eht///////', 'hcnerf////', 'noitugover', 'tsliht////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////']\n",
      "Minibatch perplexity: 4505527906.17\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "saving 2000\n",
      "Average loss at step 2200: 0.189764 learning rate: 4.246732\n",
      "['/////////a', 'pejorative', '///////way', '////////to', '//describe', '///////any', '///////act', '//////that', '//////used', '///violent', '/////means', '////////to', '///destroy', '///////the', 'organizati', '////////of', '///society', '////////it', '///////has', '//////also', '//////been', '/////taken', '////////up', '////////as', '/////////a', '//positive', '/////label', '////////by', '//////self', '///defined', 'anarchists', '///////the', '//////word', '/anarchism', '////////is', '///derived', '//////from', '///////the', '/////greek', '///without', '///archons', '/////ruler', '/////chief', '//////king', '/anarchism', '////////as', '/////////a', '/political', 'philosophy', '////////is', '///////the', '////belief', '//////that', '////rulers', '///////are', 'unnecessar', '///////and', '////should', '////////be', '/abolished', '//although', '/////there', '///////are', '/differing', 'interpreta', '////////of', '//////what', '//////this', '/////means', '/anarchism', '//////also', '////refers', '////////to', '///related', '////social', '/movements', '//////that', '//advocate', '///////the', 'eliminatio', '////////of', 'authoritar', 'institutio', 'particular', '///////the', '/////state', '///////the', '//////word', '///anarchy', '////////as', '//////most', 'anarchists', '///////use', '////////it', '//////does', '///////not', '/////imply', '/////chaos', '//nihilism', '////////or']\n",
      "['a/////////', 'evitarojep', 'yaw///////', 'ot////////', 'ebircsed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tneloiv///', 'snaem/////', 'ot////////', 'yortsed///', 'eht///////', 'itazinagro', 'fo////////', 'yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evitisop//', 'lebal/////', 'yb////////', 'fles//////', 'denifed///', 'stsihcrana', 'eht///////', 'drow//////', 'msihcrana/', 'si////////', 'devired///', 'morf//////', 'eht///////', 'keerg/////', 'tuohtiw///', 'snohcra///', 'relur/////', 'feihc/////', 'gnik//////', 'msihcrana/', 'sa////////', 'a/////////', 'lacitilop/', 'yhposolihp', 'si////////', 'eht///////', 'feileb////', 'taht//////', 'srelur////', 'era///////', 'rassecennu', 'dna///////', 'dluohs////', 'eb////////', 'dehsiloba/', 'hguohtla//', 'ereht/////', 'era///////', 'gnireffid/', 'aterpretni', 'fo////////', 'tahw//////', 'siht//////', 'snaem/////', 'msihcrana/', 'osla//////', 'srefer////', 'ot////////', 'detaler///', 'laicos////', 'stnemevom/', 'taht//////', 'etacovda//', 'eht///////', 'oitanimile', 'fo////////', 'ratirohtua', 'oitutitsni', 'ralucitrap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcrana///', 'sa////////', 'tsom//////', 'stsihcrana', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylpmi/////', 'soahc/////', 'msilihin//', 'ro////////']\n",
      "['a/////////', 'evitaropes', 'yaw///////', 'ot////////', 'ebirosed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tnelovv///', 'snamm/////', 'ot////////', 'yortsed///', 'eht///////', 'itatinagro', 'fo////////', 'ytescos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evisosopx/', 'leba//////', 'yb////////', 'ffes//////', 'denifed///', 'stsihrra//', 'eht///////', 'drow//////', 'msihcaama/', 'si////////', 'devired///', 'morf//////', 'eht///////', 'keerg/////', 'tuohtiw///', 'snohcrac//', 'relur/////', 'feihc/////', 'gnik//////', 'msihcaama/', 'sa////////', 'a/////////', 'lacitipop/', 'yhposolihp', 'si////////', 'eht///////', 'feileb////', 'taht//////', 'srelup////', 'era///////', 'ruuuucuunu', 'dna///////', 'dluohs////', 'eb////////', 'dehsilabal', 'hguohtla//', 'ereht/////', 'era///////', 'gnifeffi//', 'apsrpeepni', 'fo////////', 'tahw//////', 'siht//////', 'snamm/////', 'msihcaama/', 'osla//////', 'srefere///', 'ot////////', 'detaler///', 'laicos////', 'stnemovom/', 'taht//////', 'etacovea//', 'eht///////', 'oitanemele', 'fo////////', 'ratirohtua', 'oitutnssni', 'ralucirrap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcnana///', 'sa////////', 'tsom//////', 'stsihrra//', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'yllmi/////', 'sohhc/////', 'msiddhdd//', 'ro////////']\n",
      "Minibatch perplexity: 166368558.10\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "saving 2200\n",
      "Average loss at step 2400: 0.208460 learning rate: 4.246732\n",
      "['////anomie', '///////but', '////rather', '/////////a', 'harmonious', '//////anti', 'authoritar', '///society', '////////in', '/////place', '////////of', '//////what', '///////are', '//regarded', '////////as', 'authoritar', '/political', 'structures', '///////and', '//coercive', '//economic', '/instituti', '/anarchism', 'originated', '////////as', '/////////a', '//////term', '////////of', '/////abuse', '/////first', '//////used', '///against', '/////early', '///working', '/////class', '//radicals', '/including', '///////the', '///diggers', '////////of', '///////the', '///english', 'revolution', '///////and', '///////the', '//////sans', '//culottes', '////////of', '///////the', '////french', 'revolution', '////whilst', '///////the', '//////term', '////////is', '/////still', '//////used', '////////in', '/////////a', 'pejorative', '///////way', '////////to', '//describe', '///////any', '///////act', '//////that', '//////used', '///violent', '/////means', '////////to', '///destroy', '///////the', 'organizati', '////////of', '///society', '////////it', '///////has', '//////also', '//////been', '/////taken', '////////up', '////////as', '/////////a', '//positive', '/////label', '////////by', '//////self', '///defined', 'anarchists', '///////the', '//////word', '/anarchism', '////////is', '///derived', '//////from', '///////the', '/////greek', '///without', '///archons', '/////ruler']\n",
      "['eimona////', 'tub///////', 'rehtar////', 'a/////////', 'suoinomrah', 'itna//////', 'ratirohtua', 'yteicos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'dedrager//', 'sa////////', 'ratirohtua', 'lacitilop/', 'serutcurts', 'dna///////', 'evicreoc//', 'cimonoce//', 'itutitsni/', 'msihcrana/', 'detanigiro', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///', 'ylrae/////', 'gnikrow///', 'ssalc/////', 'slacidar//', 'gnidulcni/', 'eht///////', 'sreggid///', 'fo////////', 'eht///////', 'hsilgne///', 'noitulover', 'dna///////', 'eht///////', 'snas//////', 'settoluc//', 'fo////////', 'eht///////', 'hcnerf////', 'noitulover', 'tslihw////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////', 'a/////////', 'evitarojep', 'yaw///////', 'ot////////', 'ebircsed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tneloiv///', 'snaem/////', 'ot////////', 'yortsed///', 'eht///////', 'itazinagro', 'fo////////', 'yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evitisop//', 'lebal/////', 'yb////////', 'fles//////', 'denifed///', 'stsihcrana', 'eht///////', 'drow//////', 'msihcrana/', 'si////////', 'devired///', 'morf//////', 'eht///////', 'keerg/////', 'tuohtiw///', 'snohcra///', 'relur/////']\n",
      "['eimmon////', 'tub///////', 'rehtar////', 'a/////////', 'suonnrrrah', 'itnia/////', 'raiirohtua', 'yteicos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'degraee///', 'sa////////', 'raiirohtua', 'lacitisop/', 'serutcusts', 'dna///////', 'eviceefc//', 'cnmenoce//', 'itutitsni/', 'msincnama/', 'detanigicc', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'ebuta/////', 'tsrif/////', 'desu//////', 'nsniaga///', 'ylrae/////', 'gniwrow///', 'ssalc/////', 'slacirrr//', 'gnidulcni/', 'eht///////', 'sregdid///', 'fo////////', 'eht///////', 'hsillne///', 'noitulovrr', 'dna///////', 'eht///////', 'snas//////', 'seutaltt//', 'fo////////', 'eht///////', 'hcnerf////', 'noitulovrr', 'tsliht////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////', 'a/////////', 'evitarepop', 'yaw///////', 'ot////////', 'ebircsed//', 'yna///////', 'tcc///////', 'taht//////', 'desu//////', 'tneloiv///', 'snaem/////', 'ot////////', 'yortsero//', 'eht///////', 'itazinagio', 'fo////////', 'yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evitosop//', 'lebab/////', 'yb////////', 'fles//////', 'denieed///', 'stsircra//', 'eht///////', 'drow//////', 'msincnama/', 'si////////', 'devired///', 'morf//////', 'eht///////', 'keerg/////', 'tuohtiw///', 'snohcra///', 'relrr/////']\n",
      "Minibatch perplexity: 239263267.38\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "Average loss at step 2600: 0.169550 learning rate: 4.076863\n",
      "['/////chief', '//////king', '/anarchism', '////////as', '/////////a', '/political', 'philosophy', '////////is', '///////the', '////belief', '//////that', '////rulers', '///////are', 'unnecessar', '///////and', '////should', '////////be', '/abolished', '//although', '/////there', '///////are', '/differing', 'interpreta', '////////of', '//////what', '//////this', '/////means', '/anarchism', '//////also', '////refers', '////////to', '///related', '////social', '/movements', '//////that', '//advocate', '///////the', 'eliminatio', '////////of', 'authoritar', 'institutio', 'particular', '///////the', '/////state', '///////the', '//////word', '///anarchy', '////////as', '//////most', 'anarchists', '///////use', '////////it', '//////does', '///////not', '/////imply', '/////chaos', '//nihilism', '////////or', '////anomie', '///////but', '////rather', '/////////a', 'harmonious', '//////anti', 'authoritar', '///society', '////////in', '/////place', '////////of', '//////what', '///////are', '//regarded', '////////as', 'authoritar', '/political', 'structures', '///////and', '//coercive', '//economic', '/instituti', '/anarchism', 'originated', '////////as', '/////////a', '//////term', '////////of', '/////abuse', '/////first', '//////used', '///against', '/////early', '///working', '/////class', '//radicals', '/including', '///////the', '///diggers', '////////of', '///////the', '///english']\n",
      "['feihc/////', 'gnik//////', 'msihcrana/', 'sa////////', 'a/////////', 'lacitilop/', 'yhposolihp', 'si////////', 'eht///////', 'feileb////', 'taht//////', 'srelur////', 'era///////', 'rassecennu', 'dna///////', 'dluohs////', 'eb////////', 'dehsiloba/', 'hguohtla//', 'ereht/////', 'era///////', 'gnireffid/', 'aterpretni', 'fo////////', 'tahw//////', 'siht//////', 'snaem/////', 'msihcrana/', 'osla//////', 'srefer////', 'ot////////', 'detaler///', 'laicos////', 'stnemevom/', 'taht//////', 'etacovda//', 'eht///////', 'oitanimile', 'fo////////', 'ratirohtua', 'oitutitsni', 'ralucitrap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcrana///', 'sa////////', 'tsom//////', 'stsihcrana', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylpmi/////', 'soahc/////', 'msilihin//', 'ro////////', 'eimona////', 'tub///////', 'rehtar////', 'a/////////', 'suoinomrah', 'itna//////', 'ratirohtua', 'yteicos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'dedrager//', 'sa////////', 'ratirohtua', 'lacitilop/', 'serutcurts', 'dna///////', 'evicreoc//', 'cimonoce//', 'itutitsni/', 'msihcrana/', 'detanigiro', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///', 'ylrae/////', 'gnikrow///', 'ssalc/////', 'slacidar//', 'gnidulcni/', 'eht///////', 'sreggid///', 'fo////////', 'eht///////', 'hsilgne///']\n",
      "['feihc/////', 'gnik//////', 'msihcram//', 'sa////////', 'a/////////', 'lacitilop/', 'yhposolidp', 'si////////', 'eht///////', 'feileb////', 'taht//////', 'srelup////', 'era///////', 'reesec/ene', 'dna///////', 'dluoss////', 'eb////////', 'dehsilabat', 'hguohtla//', 'ereht/////', 'era///////', 'gnirefeid/', 'apsepeepte', 'fo////////', 'tahw//////', 'siht//////', 'snaem/////', 'msincrama/', 'osla//////', 'srefer////', 'ot////////', 'detaler///', 'laicos////', 'stnemev/m/', 'taht//////', 'etacov/a//', 'eht///////', 'oitanelele', 'fo////////', 'ririrohuua', 'oitutitsni', 'ralucitrap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcrana///', 'sa////////', 'tsom//////', 'stsincran/', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'lllmi/////', 'soohc/////', 'msilih/n//', 'ro////////', 'eimono////', 'tub///////', 'rehtar////', 'a/////////', 'suonnamrah', 'itna//////', 'ratirohtua', 'yteicos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'degraeeb//', 'sa////////', 'ririrohtua', 'lacitilop/', 'serutcusts', 'dna///////', 'evicreff//', 'cimococ///', 'itutitsnu/', 'msincrama/', 'detanigaco', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///', 'ylrae/////', 'gniwrow///', 'ssalc/////', 'slacidrrt/', 'gnidulcni/', 'eht///////', 'sreddid///', 'fo////////', 'eht///////', 'hsilgne///']\n",
      "Minibatch perplexity: 3315286673.14\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "Average loss at step 2800: 0.164844 learning rate: 4.076863\n",
      "['revolution', '///////and', '///////the', '//////sans', '//culottes', '////////of', '///////the', '////french', 'revolution', '////whilst', '///////the', '//////term', '////////is', '/////still', '//////used', '////////in', '/////////a', 'pejorative', '///////way', '////////to', '//describe', '///////any', '///////act', '//////that', '//////used', '///violent', '/////means', '////////to', '///destroy', '///////the', 'organizati', '////////of', '///society', '////////it', '///////has', '//////also', '//////been', '/////taken', '////////up', '////////as', '/////////a', '//positive', '/////label', '////////by', '//////self', '///defined', 'anarchists', '///////the', '//////word', '/anarchism', '////////is', '///derived', '//////from', '///////the', '/////greek', '///without', '///archons', '/////ruler', '/////chief', '//////king', '/anarchism', '////////as', '/////////a', '/political', 'philosophy', '////////is', '///////the', '////belief', '//////that', '////rulers', '///////are', 'unnecessar', '///////and', '////should', '////////be', '/abolished', '//although', '/////there', '///////are', '/differing', 'interpreta', '////////of', '//////what', '//////this', '/////means', '/anarchism', '//////also', '////refers', '////////to', '///related', '////social', '/movements', '//////that', '//advocate', '///////the', 'eliminatio', '////////of', 'authoritar', 'institutio', 'particular']\n",
      "['noitulover', 'dna///////', 'eht///////', 'snas//////', 'settoluc//', 'fo////////', 'eht///////', 'hcnerf////', 'noitulover', 'tslihw////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////', 'a/////////', 'evitarojep', 'yaw///////', 'ot////////', 'ebircsed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tneloiv///', 'snaem/////', 'ot////////', 'yortsed///', 'eht///////', 'itazinagro', 'fo////////', 'yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evitisop//', 'lebal/////', 'yb////////', 'fles//////', 'denifed///', 'stsihcrana', 'eht///////', 'drow//////', 'msihcrana/', 'si////////', 'devired///', 'morf//////', 'eht///////', 'keerg/////', 'tuohtiw///', 'snohcra///', 'relur/////', 'feihc/////', 'gnik//////', 'msihcrana/', 'sa////////', 'a/////////', 'lacitilop/', 'yhposolihp', 'si////////', 'eht///////', 'feileb////', 'taht//////', 'srelur////', 'era///////', 'rassecennu', 'dna///////', 'dluohs////', 'eb////////', 'dehsiloba/', 'hguohtla//', 'ereht/////', 'era///////', 'gnireffid/', 'aterpretni', 'fo////////', 'tahw//////', 'siht//////', 'snaem/////', 'msihcrana/', 'osla//////', 'srefer////', 'ot////////', 'detaler///', 'laicos////', 'stnemevom/', 'taht//////', 'etacovda//', 'eht///////', 'oitanimile', 'fo////////', 'ratirohtua', 'oitutitsni', 'ralucitrap']\n",
      "['noitiliver', 'dna///////', 'eht///////', 'snas//////', 'seulllut//', 'fo////////', 'eht///////', 'hcnerf////', 'noitiliver', 'tsliht////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////', 'a/////////', 'evitarppep', 'yaw///////', 'ot////////', 'ebircsed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tnevoiv///', 'snaem/////', 'ot////////', 'yottser///', 'eht///////', 'itazinagro', 'fo////////', 'yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evitossp//', 'leba//////', 'yb////////', 'fles//////', 'denifed///', 'stsincra//', 'eht///////', 'drow//////', 'msincrama/', 'si////////', 'devired///', 'morf//////', 'eht///////', 'keer//////', 'tuohtiw///', 'snohsra///', 'relrr/////', 'feihci////', 'gnik//////', 'msincrama/', 'sa////////', 'a/////////', 'lacitilop/', 'yhgaosmois', 'si////////', 'eht///////', 'feileb////', 'taht//////', 'srelrr////', 'era///////', 'rausucuuuu', 'dna///////', 'dluohs////', 'eb////////', 'dehiilaba/', 'hguohtla//', 'ereht/////', 'era///////', 'gnireffid/', 'atirtretni', 'fo////////', 'tahw//////', 'siht//////', 'snaem/////', 'msincrama/', 'osla//////', 'srefer////', 'ot////////', 'detaler///', 'laicos////', 'stnemevom/', 'taht//////', 'etaco/ea//', 'eht///////', 'oitanimele', 'fo////////', 'ratirohtua', 'oitutitsni', 'ralucitrap']\n",
      "Minibatch perplexity: 242135896.68\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "Average loss at step 3000: 0.142523 learning rate: 3.913789\n",
      "['///////the', '/////state', '///////the', '//////word', '///anarchy', '////////as', '//////most', 'anarchists', '///////use', '////////it', '//////does', '///////not', '/////imply', '/////chaos', '//nihilism', '////////or', '////anomie', '///////but', '////rather', '/////////a', 'harmonious', '//////anti', 'authoritar', '///society', '////////in', '/////place', '////////of', '//////what', '///////are', '//regarded', '////////as', 'authoritar', '/political', 'structures', '///////and', '//coercive', '//economic', '/instituti', '/anarchism', 'originated', '////////as', '/////////a', '//////term', '////////of', '/////abuse', '/////first', '//////used', '///against', '/////early', '///working', '/////class', '//radicals', '/including', '///////the', '///diggers', '////////of', '///////the', '///english', 'revolution', '///////and', '///////the', '//////sans', '//culottes', '////////of', '///////the', '////french', 'revolution', '////whilst', '///////the', '//////term', '////////is', '/////still', '//////used', '////////in', '/////////a', 'pejorative', '///////way', '////////to', '//describe', '///////any', '///////act', '//////that', '//////used', '///violent', '/////means', '////////to', '///destroy', '///////the', 'organizati', '////////of', '///society', '////////it', '///////has', '//////also', '//////been', '/////taken', '////////up', '////////as', '/////////a', '//positive']\n",
      "['eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcrana///', 'sa////////', 'tsom//////', 'stsihcrana', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylpmi/////', 'soahc/////', 'msilihin//', 'ro////////', 'eimona////', 'tub///////', 'rehtar////', 'a/////////', 'suoinomrah', 'itna//////', 'ratirohtua', 'yteicos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'dedrager//', 'sa////////', 'ratirohtua', 'lacitilop/', 'serutcurts', 'dna///////', 'evicreoc//', 'cimonoce//', 'itutitsni/', 'msihcrana/', 'detanigiro', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///', 'ylrae/////', 'gnikrow///', 'ssalc/////', 'slacidar//', 'gnidulcni/', 'eht///////', 'sreggid///', 'fo////////', 'eht///////', 'hsilgne///', 'noitulover', 'dna///////', 'eht///////', 'snas//////', 'settoluc//', 'fo////////', 'eht///////', 'hcnerf////', 'noitulover', 'tslihw////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////', 'a/////////', 'evitarojep', 'yaw///////', 'ot////////', 'ebircsed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tneloiv///', 'snaem/////', 'ot////////', 'yortsed///', 'eht///////', 'itazinagro', 'fo////////', 'yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evitisop//']\n",
      "['eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcnana///', 'sa////////', 'tsom//////', 'stsircra//', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylpmi/////', 'ssohc/////', 'msidddwin/', 'ro////////', 'eimono////', 'tub///////', 'rehtar////', 'a/////////', 'suonnamrah', 'itni//////', 'ratihohtua', 'yteicos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'derrager//', 'sa////////', 'ratihohtua', 'lacitilop/', 'secutsur/s', 'dna///////', 'eviceeff//', 'cimonoce//', 'itutitsni/', 'msincaana/', 'detanigarc', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///', 'ylrae/////', 'gnikrow///', 'ssalc/////', 'slacid/r//', 'gnidulcni/', 'eht///////', 'sreddid///', 'fo////////', 'eht///////', 'hsilnne///', 'noitulover', 'dna///////', 'eht///////', 'snas//////', 'seulaluq//', 'fo////////', 'eht///////', 'hcnerf////', 'noitulover', 'tsliht////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////', 'a/////////', 'evitaroppp', 'yaw///////', 'ot////////', 'ebircsed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tnevovv///', 'snaem/////', 'ot////////', 'yortsed///', 'eht///////', 'itazinagro', 'fo////////', 'yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evitisop//']\n",
      "Minibatch perplexity: 6152128.89\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "saving 3000\n",
      "Average loss at step 3200: 0.129465 learning rate: 3.913789\n",
      "['/////label', '////////by', '//////self', '///defined', 'anarchists', '///////the', '//////word', '/anarchism', '////////is', '///derived', '//////from', '///////the', '/////greek', '///without', '///archons', '/////ruler', '/////chief', '//////king', '/anarchism', '////////as', '/////////a', '/political', 'philosophy', '////////is', '///////the', '////belief', '//////that', '////rulers', '///////are', 'unnecessar', '///////and', '////should', '////////be', '/abolished', '//although', '/////there', '///////are', '/differing', 'interpreta', '////////of', '//////what', '//////this', '/////means', '/anarchism', '//////also', '////refers', '////////to', '///related', '////social', '/movements', '//////that', '//advocate', '///////the', 'eliminatio', '////////of', 'authoritar', 'institutio', 'particular', '///////the', '/////state', '///////the', '//////word', '///anarchy', '////////as', '//////most', 'anarchists', '///////use', '////////it', '//////does', '///////not', '/////imply', '/////chaos', '//nihilism', '////////or', '////anomie', '///////but', '////rather', '/////////a', 'harmonious', '//////anti', 'authoritar', '///society', '////////in', '/////place', '////////of', '//////what', '///////are', '//regarded', '////////as', 'authoritar', '/political', 'structures', '///////and', '//coercive', '//economic', '/instituti', '/anarchism', 'originated', '////////as', '/////////a']\n",
      "['lebal/////', 'yb////////', 'fles//////', 'denifed///', 'stsihcrana', 'eht///////', 'drow//////', 'msihcrana/', 'si////////', 'devired///', 'morf//////', 'eht///////', 'keerg/////', 'tuohtiw///', 'snohcra///', 'relur/////', 'feihc/////', 'gnik//////', 'msihcrana/', 'sa////////', 'a/////////', 'lacitilop/', 'yhposolihp', 'si////////', 'eht///////', 'feileb////', 'taht//////', 'srelur////', 'era///////', 'rassecennu', 'dna///////', 'dluohs////', 'eb////////', 'dehsiloba/', 'hguohtla//', 'ereht/////', 'era///////', 'gnireffid/', 'aterpretni', 'fo////////', 'tahw//////', 'siht//////', 'snaem/////', 'msihcrana/', 'osla//////', 'srefer////', 'ot////////', 'detaler///', 'laicos////', 'stnemevom/', 'taht//////', 'etacovda//', 'eht///////', 'oitanimile', 'fo////////', 'ratirohtua', 'oitutitsni', 'ralucitrap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcrana///', 'sa////////', 'tsom//////', 'stsihcrana', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylpmi/////', 'soahc/////', 'msilihin//', 'ro////////', 'eimona////', 'tub///////', 'rehtar////', 'a/////////', 'suoinomrah', 'itna//////', 'ratirohtua', 'yteicos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'dedrager//', 'sa////////', 'ratirohtua', 'lacitilop/', 'serutcurts', 'dna///////', 'evicreoc//', 'cimonoce//', 'itutitsni/', 'msihcrana/', 'detanigiro', 'sa////////', 'a/////////']\n",
      "['lebab/////', 'yb////////', 'fees//////', 'denifed///', 'stsincra/a', 'eht///////', 'drow//////', 'msimcnama/', 'si////////', 'devired///', 'morf//////', 'eht///////', 'keerg/////', 'tuohtiw///', 'snohrra///', 'relur/////', 'feihc/////', 'gnik//////', 'msimcnama/', 'sa////////', 'a/////////', 'lacitisop/', 'yhposilihp', 'si////////', 'eht///////', 'feiled////', 'taht//////', 'srelrr////', 'era///////', 'raesuc/rne', 'dna///////', 'dluohs////', 'eb////////', 'delsilabab', 'hguohtla//', 'ereht/////', 'era///////', 'gnireffid/', 'atsrtnetni', 'fo////////', 'tahw//////', 'siht//////', 'snaem/////', 'msimcnama/', 'osla//////', 'srefer////', 'ot////////', 'detaler///', 'laicis////', 'stnemxvom/', 'taht//////', 'etacovaa//', 'eht///////', 'oitanimele', 'fo////////', 'ratihohtua', 'oitutitsii', 'ralucitsap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcrana///', 'sa////////', 'tsom//////', 'stsircra/a', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylsmi/////', 'soohc/////', 'isiliddd//', 'ro////////', 'eimino////', 'tub///////', 'rehtar////', 'a/////////', 'suomnamrah', 'itai//////', 'ratihohtua', 'yteicos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'dedrager//', 'sa////////', 'ratihohtua', 'lacitisop/', 'serutcusts', 'dna///////', 'evicreff//', 'cimenoce//', 'itutitsii/', 'msimcnama/', 'detanigaco', 'sa////////', 'a/////////']\n",
      "Minibatch perplexity: 1558295690.90\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "Average loss at step 3400: 0.166002 learning rate: 3.913789\n",
      "['//////term', '////////of', '/////abuse', '/////first', '//////used', '///against', '/////early', '///working', '/////class', '//radicals', '/including', '///////the', '///diggers', '////////of', '///////the', '///english', 'revolution', '///////and', '///////the', '//////sans', '//culottes', '////////of', '///////the', '////french', 'revolution', '////whilst', '///////the', '//////term', '////////is', '/////still', '//////used', '////////in', '/////////a', 'pejorative', '///////way', '////////to', '//describe', '///////any', '///////act', '//////that', '//////used', '///violent', '/////means', '////////to', '///destroy', '///////the', 'organizati', '////////of', '///society', '////////it', '///////has', '//////also', '//////been', '/////taken', '////////up', '////////as', '/////////a', '//positive', '/////label', '////////by', '//////self', '///defined', 'anarchists', '///////the', '//////word', '/anarchism', '////////is', '///derived', '//////from', '///////the', '/////greek', '///without', '///archons', '/////ruler', '/////chief', '//////king', '/anarchism', '////////as', '/////////a', '/political', 'philosophy', '////////is', '///////the', '////belief', '//////that', '////rulers', '///////are', 'unnecessar', '///////and', '////should', '////////be', '/abolished', '//although', '/////there', '///////are', '/differing', 'interpreta', '////////of', '//////what', '//////this']\n",
      "['mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///', 'ylrae/////', 'gnikrow///', 'ssalc/////', 'slacidar//', 'gnidulcni/', 'eht///////', 'sreggid///', 'fo////////', 'eht///////', 'hsilgne///', 'noitulover', 'dna///////', 'eht///////', 'snas//////', 'settoluc//', 'fo////////', 'eht///////', 'hcnerf////', 'noitulover', 'tslihw////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////', 'a/////////', 'evitarojep', 'yaw///////', 'ot////////', 'ebircsed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tneloiv///', 'snaem/////', 'ot////////', 'yortsed///', 'eht///////', 'itazinagro', 'fo////////', 'yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evitisop//', 'lebal/////', 'yb////////', 'fles//////', 'denifed///', 'stsihcrana', 'eht///////', 'drow//////', 'msihcrana/', 'si////////', 'devired///', 'morf//////', 'eht///////', 'keerg/////', 'tuohtiw///', 'snohcra///', 'relur/////', 'feihc/////', 'gnik//////', 'msihcrana/', 'sa////////', 'a/////////', 'lacitilop/', 'yhposolihp', 'si////////', 'eht///////', 'feileb////', 'taht//////', 'srelur////', 'era///////', 'rassecennu', 'dna///////', 'dluohs////', 'eb////////', 'dehsiloba/', 'hguohtla//', 'ereht/////', 'era///////', 'gnireffid/', 'aterpretni', 'fo////////', 'tahw//////', 'siht//////']\n",
      "['mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///', 'ylrae/////', 'gniwrow///', 'ssalc/////', 'slacidrr//', 'gnidnllni/', 'eht///////', 'sreirid///', 'fo////////', 'eht///////', 'hsilnne///', 'noitulover', 'dna///////', 'eht///////', 'snas//////', 'seululuq//', 'fo////////', 'eht///////', 'hcnerf////', 'noitulover', 'tsliht////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////', 'a/////////', 'evitaropep', 'yaw///////', 'ot////////', 'ebircsed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tnenoivn//', 'snaem/////', 'ot////////', 'yortsed///', 'eht///////', 'itazinagro', 'fo////////', 'eteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evisisepm/', 'lebabb////', 'yb////////', 'fles//////', 'denifid///', 'stsincna/a', 'eht///////', 'drow//////', 'msnncnana/', 'si////////', 'devirdd///', 'morf//////', 'eht///////', 'keerg/////', 'tuohtiw///', 'snorrra///', 'relrr/////', 'feihc/////', 'gnik//////', 'msnncnana/', 'sa////////', 'a/////////', 'lacitisop/', 'yhpililihp', 'si////////', 'eht///////', 'feilbb////', 'taht//////', 'srelur////', 'era///////', 'ruusuc/unu', 'dna///////', 'dluohs////', 'eb////////', 'delsilabab', 'hguohtla//', 'ereht/////', 'era///////', 'gnireffid/', 'atirp/etti', 'fo////////', 'tahw//////', 'siht//////']\n",
      "Minibatch perplexity: 33691063.74\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "Average loss at step 3600: 0.149571 learning rate: 3.757237\n",
      "['/////means', '/anarchism', '//////also', '////refers', '////////to', '///related', '////social', '/movements', '//////that', '//advocate', '///////the', 'eliminatio', '////////of', 'authoritar', 'institutio', 'particular', '///////the', '/////state', '///////the', '//////word', '///anarchy', '////////as', '//////most', 'anarchists', '///////use', '////////it', '//////does', '///////not', '/////imply', '/////chaos', '//nihilism', '////////or', '////anomie', '///////but', '////rather', '/////////a', 'harmonious', '//////anti', 'authoritar', '///society', '////////in', '/////place', '////////of', '//////what', '///////are', '//regarded', '////////as', 'authoritar', '/political', 'structures', '///////and', '//coercive', '//economic', '/instituti', '/anarchism', 'originated', '////////as', '/////////a', '//////term', '////////of', '/////abuse', '/////first', '//////used', '///against', '/////early', '///working', '/////class', '//radicals', '/including', '///////the', '///diggers', '////////of', '///////the', '///english', 'revolution', '///////and', '///////the', '//////sans', '//culottes', '////////of', '///////the', '////french', 'revolution', '////whilst', '///////the', '//////term', '////////is', '/////still', '//////used', '////////in', '/////////a', 'pejorative', '///////way', '////////to', '//describe', '///////any', '///////act', '//////that', '//////used', '///violent']\n",
      "['snaem/////', 'msihcrana/', 'osla//////', 'srefer////', 'ot////////', 'detaler///', 'laicos////', 'stnemevom/', 'taht//////', 'etacovda//', 'eht///////', 'oitanimile', 'fo////////', 'ratirohtua', 'oitutitsni', 'ralucitrap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcrana///', 'sa////////', 'tsom//////', 'stsihcrana', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylpmi/////', 'soahc/////', 'msilihin//', 'ro////////', 'eimona////', 'tub///////', 'rehtar////', 'a/////////', 'suoinomrah', 'itna//////', 'ratirohtua', 'yteicos///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'dedrager//', 'sa////////', 'ratirohtua', 'lacitilop/', 'serutcurts', 'dna///////', 'evicreoc//', 'cimonoce//', 'itutitsni/', 'msihcrana/', 'detanigiro', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///', 'ylrae/////', 'gnikrow///', 'ssalc/////', 'slacidar//', 'gnidulcni/', 'eht///////', 'sreggid///', 'fo////////', 'eht///////', 'hsilgne///', 'noitulover', 'dna///////', 'eht///////', 'snas//////', 'settoluc//', 'fo////////', 'eht///////', 'hcnerf////', 'noitulover', 'tslihw////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////', 'a/////////', 'evitarojep', 'yaw///////', 'ot////////', 'ebircsed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tneloiv///']\n",
      "['snaem/////', 'msincran//', 'osla//////', 'srefer////', 'ot////////', 'detaler///', 'laicil////', 'stnemevom/', 'taht//////', 'etacovaa//', 'eht///////', 'oitanimele', 'fo////////', 'ratilolpnl', 'oititissni', 'ralucitsap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yccnana///', 'sa////////', 'tsom//////', 'stsihcra/a', 'esu///////', 'ti////////', 'sood//////', 'ton///////', 'ylpmi/////', 'sohhc/////', 'msinih/n//', 'ro////////', 'mimona////', 'tub///////', 'rehtar////', 'a/////////', 'suoinamrah', 'itna//////', 'ratiuohuua', 'ytescss///', 'ni////////', 'ecalp/////', 'fo////////', 'tahw//////', 'era///////', 'derrege///', 'sa////////', 'ratirulpul', 'lacitippp/', 'serutcrrts', 'dna///////', 'evoceedc//', 'cimenece//', 'ititissni/', 'miihcran//', 'detanigico', 'sa////////', 'a/////////', 'mret//////', 'fo////////', 'esuba/////', 'tsrif/////', 'desu//////', 'tsniaga///', 'ylrae/////', 'gnikrow///', 'asalc/////', 'slacidrra/', 'gnidulcni/', 'eht///////', 'sreddid///', 'fo////////', 'eht///////', 'hsilgne///', 'noitilover', 'dna///////', 'eht///////', 'snas//////', 'sellalut//', 'fo////////', 'eht///////', 'hcnerf////', 'noitilover', 'tsliht////', 'eht///////', 'mret//////', 'si////////', 'llits/////', 'desu//////', 'ni////////', 'a/////////', 'evitarop/p', 'yaw///////', 'ot////////', 'ebircsed//', 'yna///////', 'tca///////', 'taht//////', 'desu//////', 'tneloil///']\n",
      "Minibatch perplexity: 216261824.27\n",
      "G:\\PythonProjects\\udacity_deep_learning\n",
      "Average loss at step 3800: 0.140125 learning rate: 3.757237\n",
      "['/////means', '////////to', '///destroy', '///////the', 'organizati', '////////of', '///society', '////////it', '///////has', '//////also', '//////been', '/////taken', '////////up', '////////as', '/////////a', '//positive', '/////label', '////////by', '//////self', '///defined', 'anarchists', '///////the', '//////word', '/anarchism', '////////is', '///derived', '//////from', '///////the', '/////greek', '///without', '///archons', '/////ruler', '/////chief', '//////king', '/anarchism', '////////as', '/////////a', '/political', 'philosophy', '////////is', '///////the', '////belief', '//////that', '////rulers', '///////are', 'unnecessar', '///////and', '////should', '////////be', '/abolished', '//although', '/////there', '///////are', '/differing', 'interpreta', '////////of', '//////what', '//////this', '/////means', '/anarchism', '//////also', '////refers', '////////to', '///related', '////social', '/movements', '//////that', '//advocate', '///////the', 'eliminatio', '////////of', 'authoritar', 'institutio', 'particular', '///////the', '/////state', '///////the', '//////word', '///anarchy', '////////as', '//////most', 'anarchists', '///////use', '////////it', '//////does', '///////not', '/////imply', '/////chaos', '//nihilism', '////////or', '////anomie', '///////but', '////rather', '/////////a', 'harmonious', '//////anti', 'authoritar', '///society', '////////in', '/////place']\n",
      "['snaem/////', 'ot////////', 'yortsed///', 'eht///////', 'itazinagro', 'fo////////', 'yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evitisop//', 'lebal/////', 'yb////////', 'fles//////', 'denifed///', 'stsihcrana', 'eht///////', 'drow//////', 'msihcrana/', 'si////////', 'devired///', 'morf//////', 'eht///////', 'keerg/////', 'tuohtiw///', 'snohcra///', 'relur/////', 'feihc/////', 'gnik//////', 'msihcrana/', 'sa////////', 'a/////////', 'lacitilop/', 'yhposolihp', 'si////////', 'eht///////', 'feileb////', 'taht//////', 'srelur////', 'era///////', 'rassecennu', 'dna///////', 'dluohs////', 'eb////////', 'dehsiloba/', 'hguohtla//', 'ereht/////', 'era///////', 'gnireffid/', 'aterpretni', 'fo////////', 'tahw//////', 'siht//////', 'snaem/////', 'msihcrana/', 'osla//////', 'srefer////', 'ot////////', 'detaler///', 'laicos////', 'stnemevom/', 'taht//////', 'etacovda//', 'eht///////', 'oitanimile', 'fo////////', 'ratirohtua', 'oitutitsni', 'ralucitrap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcrana///', 'sa////////', 'tsom//////', 'stsihcrana', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylpmi/////', 'soahc/////', 'msilihin//', 'ro////////', 'eimona////', 'tub///////', 'rehtar////', 'a/////////', 'suoinomrah', 'itna//////', 'ratirohtua', 'yteicos///', 'ni////////', 'ecalp/////']\n",
      "['snaem/////', 'ot////////', 'yortsed///', 'eht///////', 'ataginigro', 'fo////////', 'yteicos///', 'ti////////', 'sah///////', 'osla//////', 'neeb//////', 'nekat/////', 'pu////////', 'sa////////', 'a/////////', 'evitisop//', 'lebal/////', 'yb////////', 'fles//////', 'denifed///', 'stsehceee/', 'eht///////', 'drow//////', 'msihcrama/', 'si////////', 'devirdd///', 'morf//////', 'eht///////', 'keerg/////', 'tuohwiw///', 'snohcra///', 'relrr/////', 'feihc/////', 'gnik//////', 'msihcrama/', 'sa////////', 'a/////////', 'lacitopop/', 'yhpolilohp', 'si////////', 'eht///////', 'feileb////', 'taht//////', 'srelur////', 'era///////', 'ruusucurne', 'dna///////', 'dluohs////', 'eb////////', 'dehsilaba/', 'hguohtla//', 'ereht/////', 'era///////', 'gnireffid/', 'atappeetni', 'fo////////', 'tahw//////', 'siht//////', 'snaem/////', 'msihcrama/', 'osla//////', 'srefrr////', 'ot////////', 'detaler///', 'laicis////', 'stnemovom/', 'taht//////', 'etacov/a//', 'eht///////', 'oitanimele', 'fo////////', 'ratirohuua', 'oitutissni', 'ralucitrap', 'eht///////', 'etats/////', 'eht///////', 'drow//////', 'yhcrana///', 'sa////////', 'tsom//////', 'stsihcrana', 'esu///////', 'ti////////', 'seod//////', 'ton///////', 'ylpmi/////', 'sooh//////', 'msilihim//', 'ro////////', 'eimona////', 'tub///////', 'rehtar////', 'a/////////', 'summnamruh', 'itna//////', 'ratirohuua', 'yteicos///', 'ni////////', 'ecalp/////']\n",
      "Minibatch perplexity: 8792993.74\n",
      "G:\\PythonProjects\\udacity_deep_learning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "num_steps = 4000\n",
    "summary_frequency = 200\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    saver = tf.train.Saver()\n",
    "    prev_perplexity = None\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        global_step.assign_add(1)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            valid_batch = valid_batches.next()\n",
    "            for i in range(num_unrollings):\n",
    "                feed_dict[train_data[i]] = valid_batch[i]\n",
    "            predictions = session.run([train_prediction], feed_dict=feed_dict)\n",
    "            print(batches2string(valid_batch))\n",
    "            labels = np.flip(valid_batch, axis = 0)\n",
    "            print(batches2string(labels))\n",
    "            predictions = np.reshape(predictions, [num_unrollings, batch_size, vocabulary_size])\n",
    "            print(batches2string(predictions))\n",
    "            perplexity = np.exp(logprob(predictions, labels))\n",
    "            print('Minibatch perplexity: %.2f' % float(perplexity))\n",
    "            print(os.getcwd())\n",
    "            if prev_perplexity is None:\n",
    "                prev_perplexity = perplexity\n",
    "                print('saving %d' % step)\n",
    "                saver.save(session, os.getcwd()+'/string_reversal_best')\n",
    "            if perplexity < prev_perplexity:\n",
    "                prev_perplexity = perplexity\n",
    "                print('saving %d' % step)\n",
    "                saver.save(session, os.getcwd()+'/string_reversal_best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from G:\\PythonProjects\\udacity_deep_learning/string_reversal_best\n",
      "Enter a string to reversethe quick brown fox\n",
      "['eht///////', 'kciuq/////', 'nworb/////', 'xof///////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////', '//////////']\n",
      "['eht///////', 'kciuq/////', 'nworb/////', 'xof///////']\n",
      "eht kciuq nworb xof\n",
      "Do you want to continue? (Y/N)N\n"
     ]
    }
   ],
   "source": [
    "prediction = None\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    saver = tf.train.import_meta_graph(os.getcwd()+'/string_reversal_best.meta')\n",
    "    saver.restore(sess,tf.train.latest_checkpoint(os.getcwd()+'/'))\n",
    "    cont = True\n",
    "    while cont is True:\n",
    "        st = input('Enter a string to reverse')\n",
    "        st = st.split(' ')\n",
    "        words_scaled = [(num_unrollings - len(word)) * '/' + word if len(word) < num_unrollings else word[:num_unrollings] for word in st]\n",
    "        final_batch = list()\n",
    "        for j in range(num_unrollings):\n",
    "            batches = np.zeros([100, vocabulary_size])\n",
    "            for i in range(len(words_scaled)):\n",
    "                batches[i, char2id(words_scaled[i][j])] = 1\n",
    "            final_batch.append(batches)\n",
    "        final_batch = np.reshape(final_batch, [num_unrollings, 100, vocabulary_size])\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[train_data[i]] = final_batch[i]\n",
    "        prediction = sess.run([train_prediction], feed_dict=feed_dict)\n",
    "        labels = np.flip(final_batch, axis = 0)\n",
    "        print(batches2string(labels))\n",
    "        prediction = np.reshape(prediction, [num_unrollings, 100, vocabulary_size])\n",
    "        print(batches2string(prediction[:, 0:len(words_scaled), :]))\n",
    "        words = batches2string(prediction[:, 0:len(words_scaled), :])\n",
    "        fin_str = ''\n",
    "        for word in words:\n",
    "            while '/' in word:\n",
    "                word = word.replace('/','')\n",
    "            if fin_str is '':\n",
    "                fin_str = word\n",
    "            else:\n",
    "                fin_str = fin_str + ' ' + word\n",
    "        print(fin_str)\n",
    "        ip = input('Do you want to continue? (Y/N)')\n",
    "        if ip is 'N':\n",
    "            cont = False"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
